<%
  require "../eruby_util.rb"
%>

<%
  chapter(
    '11',
    %q{Probability distributions, variance, and radioactive decay curves},
    'ch:decay'
  )
%>

<% begin_sec("Probability distributions",0,'continuous-prob') %>
In ch.~\ref{ch:stat}, we considered random variables such as the number of gas molecules $r$ on
the right-hand side of the box in figure \figref{free-expansion}, p.~\pageref{fig:free-expansion}. This variable is discrete rather than
continuous, so we can speak meaningfully of the probability that the integer $r$ has some particular value.
On the other hand, the time $t$ at which a particular unstable nucleus decays is a
\emph{continuous} variable. For such a variable, there is an infinite number of possible
values, and the probability of any particular value is typically zero.

How do we handle this mathematically? Let's start finite and sneak up on the infinity.

Consider a throw of a die. If the die is ``honest,'' then we
expect all six values to be equally likely. Since all six
probabilities must add up to 1, then probability of any
particular value coming up must be 1/6. We can summarize
this in a graph, \figref{single-die}. Areas under the curve can be
interpreted as total probabilities. For instance, the area
under the curve from 1 to 3 is $1/6+1/6+1/6=1/2$, so the
probability of getting a result from 1 to 3 is 1/2. The
function shown on the graph is called the probability distribution.

Figure \figref{two-dice} shows the probabilities of various results
obtained by rolling two dice and adding them together, as in
the game of craps. The probabilities are not all the same.
There is a small probability of getting a two, for example,
because there is only one way to do it, by rolling a one and
then another one. The probability of rolling a seven is high
because there are six different ways to do it: 1+6, 2+5, etc.

<% marg(0) %>
<%
  fig(
    'single-die',
    %q{Probability distribution for the result of rolling a single die.}
  )
%>

\spacebetweenfigs

<%
  fig(
    'two-dice',
    %q{Rolling two dice and adding them up.}
  )
%>
<% end_marg %>

If the number of possible outcomes is large but finite, for
example the number of hairs on a dog, the graph would start
to look like a smooth curve rather than a ziggurat.

In these examples, the probability that the result will fall within some
\emph{range} is proportional to the area under the bar graph. In other words,
we're talking about an integral. Passing to the case of a continuous variable,
we use this as our definition of the concept of a \emph{probability distribution}.\index{probability distribution}
If $x$ is a random number, the probability distribution $D(x)$ is defined
so that the probability that $x$ lies between $a$ and $b$ is equal to
\begin{equation*}
  P(a \le x \le b) = \int_a^b D(x) \der x  .
\end{equation*}
You've probably heard about ``the bell curve,'' and seen people draw it with a pencil.
When they do this,  the function they're drawing is an example of one of these probability distributions. If you've heard of
the idea that an electron in an atom is like a probability cloud, what is being
described qualitatively is actually the function $D$ (which in this case depends on
three coordinates, $x$, $y$, and $z$).

Suppose that $x$ has some units such as seconds.
Then $\der x$, which represents a small change in $x$, also has units of seconds, and since
$P$ is unitless, it follows that $D$ has units of $\sunit^{-1}$. That is, $D$ represents
the probability per unit of time. The same kind of thing occurs for random variables with
other units: whatever units $x$ has, $D$ has the inverse of those units.

Normalization is the requirement that the total probability for $x$ to have
\emph{some} value must be one,
\begin{equation*}
  \int_{-\infty}^\infty D(x) dx = 1  .
\end{equation*}
For a random variable that is discrete rather than continuous, we just do a sum rather than an integral,
$\sum P(x) = 1$.

<% marg(30) %>
<%
  fig(
    'human-height',
    %q{%
      A probability distribution for height of human adults 
      (not real data).
    }
  )
%>

\spacebetweenfigs

<%
  fig(
    'human-height-tail',
    %q{Example \ref{eg:basketball}.}
  )
%>

\spacebetweenfigs

<%
  fig(
    'average',
    %q{The average of a probability distribution.}
  )
%>
<% end_marg %>

Figure \figref{human-height} shows another example, a probability distribution
for people's height. This kind of bell-shaped curve is quite common.

<% self_check('rangeofheights',<<-'SELF_CHECK'

Compare the number of people with heights in the range of
130-135 cm to the number in the range 135-140.
  SELF_CHECK
  ) %>

%%%%%%%%%%% basketball example %%%%%%%%%%%%%
m4_include(../share/quantum/eg/basketball.tex)
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The average value of $x$ is given by
\begin{equation*}
  (\text{average of $x$}) = \langle x \rangle = \bar{x} = \int_a^b x D(x) dx  .
\end{equation*}
The notation $\langle\ldots\rangle$ means ``the average of \ldots,''
and the bar in $\bar{x}$ means the same thing.
We can think of the average of a probability distribution
geometrically as the horizontal position at which it could
be balanced if it was constructed out of cardboard, figure \figref{average}.
For a discrete variable, we again just switch the integral to a probability-weighted sum,
$\bar{x}=\sum xP(x)$.

The average is not the only possible way to say what is a
typical value for a quantity that can vary randomly; another
possible definition is the median,\index{median} defined as the value that
is exceeded with 50\% probability. When discussing incomes
of people living in a certain town, the average could be
very misleading, since it can be affected massively if a
single resident of the town is Mark Zuckerberg.

<% end_sec('continuous-prob') %>

<% begin_sec("The variance and standard deviation",nil,'variance') %>
If the next Martian you meet asks you, ``How tall is an
adult human?,'' you will probably reply with a statement
about the average human height, such as ``Oh, about 5 feet 6
inches.'' If you wanted to explain a little more, you could
say, ``But that's only an average. Most people are somewhere
between 5 feet and 6 feet tall.'' Without bothering to draw
the relevant bell curve for your new extraterrestrial
acquaintance, you've summarized the relevant information by
giving an average and a typical range of variation.  

Just as an average is not the only way of defining a central value
of a distribution, there are many possible ways of measuring the amount
of variation about that center. But a method that is common and has nice
mathematical properties is the following. We define the \emph{variance}
of a probability distribution as follows:
\begin{equation*}
  \text{(variance of $x$)} = \langle (x-\bar{x})^2 \rangle.
\end{equation*}
In other words, we consider the difference between $x$ and its average value $\bar{x}$, and
we take the average of the square of that difference. If $x$ always had exactly its average
value, then $x-\bar{x}$ would always be zero, and the variance would be zero.
It would not make sense to define
the variance without the square, because, for example, a symmetrical probability distribution
would have a variance of zero --- the negative values of $x-\bar{x}$ would cancel the positive
ones.

The big mathematical advantage of the variance is that it is additive: the variance of $x+y$ is
the same as the sum of the variances, provided that $x$ and $y$ are not correlated to one another.
For instance, if Susan has two bartending jobs, bringing
in two different incomes $x$ and $y$, then this week's variation $x-\bar{x}$ in her tips at Hipster Lounge X is probably
not related to the variation $y-\bar{y}$ in what she gets from the tip jar at Y Bar and Grill. One
can then plug in to the definition of the variance and show that the variances do add; this works out because
the cross-term $\langle (x-\bar{x})(y-\bar{y}) \rangle$ is zero.

The only unfortunate thing about the variance is that its units aren't the same as the units
of the variable such as $x$. For example, if $x$ has units of dollars, then the variance of
$x$ has units of dollars squared. For this reason, we define the standard deviation
\begin{equation*}
  \text{(standard deviation of $x$)} = \sigma_x = \sqrt{\langle (x-\bar{x})^2 \rangle}.
\end{equation*}
When people give error bars in science experiments, or ranges of error in an opinion poll,
they are usually quoting a standard deviation. If someone gives you a number like $137\pm 5\ \zu{kg}$,
then the 5 certainly can't be the variance, since the variance would have units of $\zu{kg}^2$, not kg.
The standard deviation's name comes from its interpretation as a typical or standard amount by
which $x$ deviates from $\bar{x}$.
In the context of AC circuits, you have probably encountered the idea of an r.m.s. (root-mean-square)
value, which is exactly a standard deviation (in the case where $\bar{x}=0$).

<% marg() %>
<%
  fig(
    'two-sided-box-1-and-5-atoms',
    %q{A two-sided box with one atom (top) and five atoms (bottom).}
  )
%>
<% end_marg %>
As an example, consider the simplest possible case of the gas atoms in\label{two-sided-box-std-dev}
the two-sided box.  Let the total number of atoms be
one (figure \figref{two-sided-box-1-and-5-atoms}, top), so that the number $r$ of atoms on the right-hand side is either
0 or 1, with equal probability. By normalization, each of these
probabilities is 1/2, and $\bar{r}=1/2$ as well.  A calculation
(\note{sd-one-atom}) shows that the standard deviation is also $1/2$,
which makes sense: 1/2 is not just a typical value for how much $r$
differs from $\bar{r}$, it is \emph{always} the size of that
deviation.

We now have an easy way to estimate the sizes of fluctuations in $r$ when the number of atoms is
larger. (On p.~\pageref{two-sided-box-simple} we did this by a technique that was a lot more work.)
Say there are $n=5$ atoms, as in the bottom of figure \figref{two-sided-box-1-and-5-atoms}. If this
is an ideal gas, then the atoms don't interact with each other often enough to matter, and there should
be no correlation between finding one atom on the right and another atom there. Therefore the
variances add. The variance
in $r$ contributed by one atom is $(1/2)^2=1/4$. Therefore the total variance for 5 atoms is
$5/4$, and the standard deviation of $r$ is $\sqrt{5}/2$.

This is the justification for our claim on
p.~\pageref{two-sided-box-simple} that when $n$ is large, the fluctuations in $r$ are negligible compared to $r$.
For the relative size of the fluctuations,
we should typically have $\sigma_r/\bar{r}=(\sqrt{n}/2)/(n/2)=1/\sqrt{n}$. When $n$ is $10^{22}$, for example,
the relative size of the fluctuations should be $10^{-11}$, which is much too small to measure in any
experiment. This is similar to the idea of the ``law of averages,'' which decrees that the casino always
makes a profit by the end of the month.\label{sd-root-n}

<% end_sec('variance') %>

<% begin_sec("Errors in random counts: Poisson statistics",0,'poisson') %>
If you do a lab experiment as part of this course in which you count radioactive decays with a Geiger
counter, the number of counts $N$ in a fixed time period will have some standard deviation $\sigma_N$.
This is an example of a more generally occurring situation in statistics, which is that we
we have a large number of things that may happen, each with some small
probability, and we count them up. The total number of them that do happen, $N$, is called a Poisson (``Pwa-SAN'') random variable.
For example, the number of houses burglarized in Fullerton
this year is a Poisson random variable. When you count the number of nuclear decays in
a certain time interval, the result is Poisson. The helpful thing to know is that when
a Poisson variable has an average value $N$, its statistical uncertainty is $\sqrt{N}$.
So for example if your Geiger counter counts 100 clicks in one minute, this is $100\pm10$.
We could anticipate based on almost the same reasoning as in section \ref{sec:variance}, p.~\pageref{sd-root-n},
that the standard deviation would be proportional to $\sqrt{N}$. It just so happens that the constant of
proportionality for a Poisson random variable equals one.
<% end_sec('poisson') %>

<% begin_sec("Exponential decay",nil,'exp-decay') %>

\index{decay!exponential}\index{exponential decay}

<% begin_sec("Half-life",nil,'half-life') %>
\index{half-life}
Most people know that radioactivity ``lasts a certain amount
of time,'' but that simple statement leaves out a lot. As an
example, consider the following medical procedure used to
diagnose thyroid function. A very small quantity of the
isotope $^{131}\zu{I}$, produced in a nuclear reactor, is fed to
or injected into the patient. The body's biochemical systems
treat this artificial, radioactive isotope exactly the same
as $^{127}\zu{I}$, which is the only naturally occurring type.
(Nutritionally, iodine is a necessary trace element. Iodine
taken into the body is partly excreted, but the rest becomes
concentrated in the thyroid gland. Iodized salt has had
iodine added to it to prevent the nutritional deficiency
known as \index{goiters}goiters, in which the \index{iodine}iodine-starved
thyroid becomes swollen.) As the $^{131}\zu{I}$ undergoes beta
decay, it emits electrons, neutrinos, and gamma rays. The
gamma rays can be measured by a detector passed over the
patient's body. As the radioactive iodine becomes concentrated
in the thyroid, the amount of gamma radiation coming from
the thyroid becomes greater, and that emitted by the rest of
the body is reduced. The rate at which the iodine concentrates
in the thyroid tells the doctor about the health of the thyroid.

If you ever undergo this procedure, someone will presumably
explain a little about radioactivity to you, to allay your
fears that you will turn into the Incredible Hulk, or that
your next child will have an unusual number of limbs. Since
iodine stays in your thyroid for a long time once it gets
there, one thing you'll want to know is whether your thyroid
is going to become radioactive forever. They may just tell
you that the radioactivity ``only lasts a certain amount of
time,'' but we can now carry out a quantitative derivation
of how the radioactivity really will die out.

Let $P(t)$ be the probability that an iodine atom
will survive without decaying for a period of at least $t$.
It has been experimentally measured that half all $^{131}\zu{I}$
atoms decay in 8 hours, so we have
\begin{equation*}
                P(8\ \zu{hr})         =  0.5 .  
\end{equation*}

Now using the law of independent probabilities, the
probability of surviving for 16 hours equals the probability
of surviving for the first 8 hours multiplied by the
probability of surviving for the second 8 hours,
\begin{align*}
                P(16\ \zu{hr})         &=  0.50\times0.50  \\
                                 &=  0.25 .  
\end{align*}
Similarly we have
\begin{align*}
                P(24\ \zu{hr})         &=  0.50\times0.5\times0.5  \\
                                 &=  0.125 .  
\end{align*}
Generalizing from this pattern, the probability of surviving
for any time $t$ that is a multiple of 8 hours is
\begin{equation*}
                P(t)                 =            0.5^{t/8\ \zu{hr}} .
\end{equation*}
We now know how to find the probability of survival at
intervals of 8 hours, but what about the points in time in
between? What would be the probability of surviving for 4
hours? Well, using the law of independent probabilities again, we have
\begin{equation*}
  P(8\ \zu{hr})  =  P(4\ \zu{hr}) \times P(4\ \zu{hr}) ,  
\end{equation*}
which can be rearranged to give
\begin{align*}
  P(4\ \zu{hr})          &= \sqrt{P(8\ \zu{hr})}   \\
                                 &=  \sqrt{0.5}  \\
                                 &=  0.707 .  
\end{align*}
This is exactly what we would have found simply by plugging
in $P(t)=0.5^{t/8\ \zu{hr}}$ and ignoring the restriction to
multiples of 8 hours. Since 8 hours is the amount of time
required for half of the atoms to decay, it is known as the
half-life, written $t_{1/2}$. The general rule is then the exponential decay equation
\begin{equation*}
  P(t)  =    2^{-t/t_{1/2}}. \qquad \text{[probability of survival for time $t$]}
\end{equation*}
<% end_sec('half-life') %>

<% begin_sec("A first glimpse of quantum physics",nil,'glimpse-quantum') %>
Let's step back and think for a moment about the physics underlying this derivation.
Back around 1900, when the nucleus and radioactivity were first discovered, there
were no clear principles underlying this sort of thing. In observations, it
\emph{seemed} like nuclear decay was random, and observations \emph{seemed} to
show that a nucleus's probability of decaying during a certain time interval
was statistically independent of its previous history. Nobody had any idea \emph{why}
these things were true. One might expect that any answers to questions like these
would be very technical, and would have to await a deeper understanding of the nucleus.

Actually these facts about nuclear decay require even less detailed technical
knowledge of nuclear physics than you have from sec.~\ref{sec:nucleus}.
They arise from basic facts about quantum physics.
Let's preview these facts.

\begin{enumerate}
\item \emph{Totalitarian principle:} Everything not forbidden is compulsory. That is, if a process \emph{can} take place
         without violating a conservation law, it \emph{will} take place, with some probability.
\item \emph{Ground state:} Every system has a lower bound on its energy. This is some number such that no state has any energy lower than that number.
In many cases, there is exactly one state, called the ground state, that has the lowest energy.
\item \emph{State fundamentalism:} There is nothing more that can be known about a system than its state.
\end{enumerate}

The first principle is \emph{the} fundemental difference between quantum physics and
classical physics (i.e., all the physical theories that came before). In classical physics,
pigs can't fly, particles can't transmute themselves into other particles, and a marble locked
inside a box can't get out unless we open the lid. In quantum physics, we have to ask ourselves
whether there is some conservation law that prevents these things from happening, and if there isn't
such a law, we expect that the process will happen, albeit possibly at a rate that is too low to measure.
I suspect that flying pigs probably do violate conservation of energy (sorry, pigs), but particles do
transmute themselves in various ways, and the marble definitely can get out of the box through a process
called quantum tunneling, although we can estimate the rate, and it is very low.

The second principle, a lower bound on energies, seems to be true because we see forms of matter in our
universe that seem to be either relatively stable or even (apparently) completely stable. Examples include
the proton (if isolated rather than inside a
nucleus) and black holes. If there was always some lower-energy state to decay to, then based on the
totalitarian principle, every system would have something that it could decay to without violating
conservation of energy, and therefore no system would be stable.

The third principle says that, whatever these ``states'' are, that's all there is. There is nothing
else to know, no higher reality, no deeper insight to be gained, nothing else that can be measured
or observed about the system. This principle is what allows us to assert that a nucleus's probability
of decay is independent of its history, as assumed in our derivation of the exponential decay
equation. When we see a nucleus that's been sitting around for a while, it's normally in its ground state.
Therefore there is nothing else to know about it besides the fact that it's in its ground state.
It can't have cracks and strain that show it's about to decay, nor can we see that it must be a really
tough little nucleus because it's survived for such a long time.
<% end_sec('glimpse-quantum') %>

<% begin_sec("Calculations for exponential decay",nil,'decay-calc') %>
We'll see that all our formulas come out simpler if we state them in terms of the average
lifetime $\tau$ rather than the half-life $t_{1/2}$. These are related by $\tau=t_{1/2}/\ln 2$.

Also, it's a little awkward doing exponentials with base 2. Usually
we prefer to get everything in terms of base $e$. This has
advantages, for example, when we do calculus, because it's
easy to differentiate or integrate $e^x$, but hard to do those
things with $2^x$. Using the identity $2^x=\exp(\ln 2^x)=e^{x \ln 2}$,
we find that the probability of survival is given by
\begin{equation*}
  P(t)  =    e^{-t/\tau}. \qquad \text{[probability of survival for time $t$]}   
\end{equation*}

We would like to know the probability distribution $D(t)$ for the time at which decay occurs.
Since the survival probability is $P(t)=\int_t^\infty D(t')\der t'$, the fundamental theorem of calculus gives
$D(t)=-\der P/\der t$, or
\begin{equation*}
  D(t)  =    \frac{1}{\tau}e^{-t/\tau}. \qquad \text{[prob.~dist.~of the time of decay $t$]}
\end{equation*}
We can see that the units of this equation make sense, since the probability distribution for a random
variable with units of seconds must itself have units of inverse seconds.

If you're fiddling around with a hunk of plutonium and want to know how badly your chromosomes are
getting nuked, then you're interested in the rate of decay, i.e., the number of decays per second,
$\der N/\der t$. As the cumulative number of decays goes up, the number of survivors goes down,
so $\der N=N_0D(t)\der t$, where $N_0$ is the initial number of nuclei in your sample, and
\begin{equation*}
  \frac{\der N}{\der t}  =    \frac{N_0}{\tau}e^{-t/\tau}. \qquad \text{[rate of decay]}
\end{equation*}

Note that the three functions above are all basically the same exponential function, just with
different constant factors out in front. This is because they're integrals and derivatives
of each other, and integrating or differentiating $e^{bx}$ gives back the same function with
a factor of $b$ or $1/b$.

<% marg(-8) %>
<%
  fig(
    'technetium-injection',
    %q{A patient being injected with a radiological marker containing an excited state of ${}^{99}\zu{Tc}$.
       The syringe is surrounded with a thin layer of shielding in order to reduce the radiation exposure to the
       technician's hands. The gamma rays are relatively low in energy, so not much shielding is needed in order
       to stop almost all of them.}
  )
%>
<% end_marg %>
\begin{eg}{A heart test}
\egquestion
In a common heart test, the patient is injected with a molecule containing ${}^{99}\zu{Tc}$ (technetium-99)
molecules in an excited state. This state decays by emitting a gamma ray, with a half-life of
6.01 hours. The molecules bind to red blood cells, so a gamma-ray video camera can see the flow of blood
through the chambers of the heart. Once the chemical for the injection has been prepared, it has to be
used fairly promptly. Suppose that usable medical results require that at least 40\% of the ${}^{99}\zu{Tc}$
nuclei remain in their excited state. What is the shelf life of the chemical?

\eganswer
Once we start doing math, it's easier to work with the mean lifetime, which in this case is $\tau=t_{1/2}/\ln 2=8.67\ \zu{hr}$.
We have $P(t)  =    e^{-t/\tau}$, so taking logs of both sides gives $\ln P=-t/\tau$, and
$t=-\tau(\ln 0.40)=7.9\ \zu{hr}$. This is a little more than the half-life, which makes sense, because 0.4 is a little
less than 0.5.
\end{eg}

<%
  fig(
    'carbon-fourteen',
    %q{%
      Calibration of the $^{14}\zu{C}$ dating method using
      tree rings and artifacts whose ages were known from
      other methods. Redrawn from Emilio Segr\`{e}, \textbf{Nuclei and Particles}, 1965.
    },
    {
      'width'=>'wide'
    }
  )
%>

\begin{eg}{$^{14}\zu{C}$ \index{carbon-14 dating}Dating}
Almost all the carbon on Earth is $^{12}\zu{C}$, but not quite.
The isotope $^{14}\zu{C}$, with a half-life of 5600 years, is
produced by cosmic rays in the atmosphere. It decays
naturally, but is replenished at such a rate that the
fraction of $^{14}\zu{C}$ in the atmosphere remains constant, at
$1.3\times10^{-12}$ . Living plants and animals take in both
$^{12}\zu{C}$ and $^{14}\zu{C}$ from the atmosphere and incorporate
both into their bodies. Once the living organism dies, it no
longer takes in C atoms from the atmosphere, and the
proportion of $^{14}\zu{C}$ gradually falls off as it undergoes
radioactive decay. This effect can be used to find the age
of dead organisms, or human artifacts made from plants or
animals. Figure \figref{carbon-fourteen} on page
\pageref{fig:carbon-fourteen} shows the exponential decay
curve of $^{14}\zu{C}$ in various objects. Similar methods, using
longer-lived isotopes, provided the first firm proof that
the earth was billions of years old, not a few thousand as
some had claimed on religious grounds.
\end{eg}


\startdqs

\begin{dq}
In the medical procedure involving $^{131}\zu{I}$, why is it
the gamma rays that are detected, not the electrons or
neutrinos that are also emitted?
\end{dq}

\begin{dq}
For 1 s, Fred holds in his hands 1 kg of radioactive
stuff with a half-life of 1000 years. Ginger holds 1 kg of a
different substance, with a half-life of 1 min, for the same
amount of time. Did they place themselves in equal danger, or not?
\end{dq}

\begin{dq}
Does the half-life depend on how much of the substance
you have? Does the expected time until the sample decays
completely depend on how much of the substance you have?
\end{dq}

<% end_sec('decay-calc') %>

<% end_sec('exp-decay') %>

<% begin_notes %>

\notetext{sd-one-atom}{Standard deviation of $r$ with one atom}
\notesummary{For a single atom in a box, the standard deviation of the number of atoms on the right is 1/2.}
We have a single atom in a box, with $r=1$ if the atom is in the right half and $r=0$ otherwise.
Because $r$ is discrete, the variance $\langle (r-\bar{r})^2 \rangle$
can be computed as a probability-weighted sum,
\begin{align*}
  \text{(variance of $r$)} &= \langle \left(r-\frac{1}{2}\right)^2 \rangle \\
                           &= P(r=0)\left(0-\frac{1}{2}\right)^2 \\
                           & \qquad \qquad +P(r=1)\left(1-\frac{1}{2}\right)^2 \\
                           &= \frac{1}{4}.
\end{align*}
The standard deviation of $r$ is then $\sqrt{1/4}=1/2$, as expected.

<% end_notes %>

<% begin_hw_sec(vfill:true) %>


<% end_hw_sec %>

<% begin_lab("Probability distributions",columns:1,type:'ex') %>\label{lab:prob-dist}

Questions 1-3 involve the useful concept of the \emph{cumulative distribution} (not introduced in the text).
Let $P(x)=\int_{-\infty}^x D(x') dx'$ be the probability of finding a value
for the random number that is less than or equal to $x$. The function $P$ is referred to as
the cumulative distribution. 

1. Using the fundamental theorem of calculus, express $D$ in terms of $P$

2. Suppose that $x$ has some units such as kilograms. Use one of the relations between $D$ and $P$ to
determine the units of both functions, and then check that it also works out according to the other relation.

3. Sketch the functions $D$ and $P$ for the
following random variables: (a) a random real number that has a uniform probability
of lying anywhere in the interval from 0 to 1; (b) the time $t$ at which an atom of
a radioactive isotope decays; (c) a random number that follows the standard ``bell
curve,'' shown below, with an average value of 0 and a standard deviation of 1; (d) the result of
a die roll. You will not find that both $D$ and $P$ are well defined in all cases.

\anonymousinlinefig{../../../share/quantum/figs/bell-curve}

\emph{Pause.} I'll walk you through the following problem on the 
board. A certain electron wave confined to a box of length $L$ has a probability distribution given by
\begin{equation*}
D(x) = \begin{cases}
A \sin^2 (2\pi x/L) & \text{if} \ 0 \le x \le L \\
0             & \text{elsewhere}
\end{cases}
\end{equation*}
Sketch the function. Infer the units of $A$. What would go wrong if the sine function wasn't squared?
Determine $A$ from the requirement of normalization.

4. The earth is constantly exposed to neutrinos from outer space. Most neutrinos pass through the
entire planet without interacting, but a small fraction of them are absorbed. These absorption events
are distributed uniformly within the spherical volume of the earth, which is of radius $b$. Let $r$
be the distance from the center at which one of these events occurs, so that $0 \le r \le b$.\\
(a) Show that the probability distribution $D(r)$ is of the form $kr^p$, where $k$ and $p$ are
constants, and determine $p$. Hint: consider the volume of the infinitesimally thin shell that
stretches from $r$ to $r+dr$.\\
(b) Determine $k$.\\
(c) Find the average value of $r$.

\emph{Pause.} Let $x$ be a random variable with a known distribution $D(x)$, and let $y=f(x)$ be
a function of $x$. I'll show you how to use the chain rule to determine the probability distribution
$D^*(y)$, where the star indicates that $D^*$ is a different function from $D$.

5. A drunk guy in a bar offers you the following bet. He holds a sharp razor blade facing up,
and you toss a stick of uncooked spaghetti up in the air so that when it comes down, the blade breaks
it at a random point. The spaghetti is 1 foot long, and the break point is a random variable $x$.
He offers to pay you $y=1/x$ in units of dollars. You want to determine whether this is
a sucker bet and the guy is a scammer. Find the amount of money you should be willing to
pay in return for the expected payout.
 

<% end_lab %>


<% end_chapter() %>
