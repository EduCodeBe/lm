<%
  require "../eruby_util.rb"
%>

<% part_title("Thermodynamics and the microscopic description of matter") %>

<%
  chapter(
    '07',
    %q{Statistics, equilibrium, and energy sharing},
    'ch:stat',
    %q{A flame is used to fill a balloon with hot air.},
    {'opener'=>'hot-air-balloon-large'}
  )
%>

<% marg() %>
<%
  fig(
    'humpty-dumpty',
    %q{Humpty Dumpty.}
  )
%>
<% end_marg %>

Looking at the two drawings in figure \figref{humpty-dumpty}, you
shouldn't have any trouble figuring out which came first in time and
which happened later. This process is \emph{irreversible:\/} eggs don't
unscamble.

As we saw in sec.~\ref{sec:t-symm},
p.~\pageref{sec:t-symm}, this kind of seemingly common-sense
observation is hard to explain when we consider that the laws of
physics are completely symmetric with respect to time-reversal. Why,
then, aren't all physical processes \emph{reversible}?

There is something going on that has to do with randomness. Scrambling an
egg seems to randomize it, in the same way that shaking up a box full
of pennies makes it into some random combination of heads and tails.
If someone had carefully arranged the pennies in the box so that they
were all heads up, then a thorough shaking would eliminate that
orderly pattern, and further shaking would be unlikely to restore it.

Although this seems like a step toward an explanation of time's arrow,
it is not yet a complete explanation. For one thing, we could time-reverse
the motion of the pennies, and then they would unscramble themselves
while again obeying Newton's laws. It does seem like it would be very
difficult to set up the time-reversed motion in exactly the right way,
but why is it that that sort of thing is so difficult?

In order to pursue this train of thought, we'll need some basic ideas
about probability and statistics.

<% begin_sec("Basics of probability and statistics",nil,'basic-prob') %>

Even if something is random, we
can still understand it, and we can still calculate
probabilities numerically.

<% begin_sec("Statistical independence",nil,'independence') %>
\index{independence!statistical}
As an illustration of one general technique for calculating
probabilities, suppose you are playing a 25-cent slot machine. Each
of the three wheels has one chance in ten of coming up with
a cherry. If all three wheels come up cherries, you win
\$100. Even though the results of any particular trial are
random, you can make certain quantitative predictions.
First, you can calculate that your odds of winning on any
given trial are $1/10\times1/10\times1/10=1/1000=0.001$.\label{slot-machine-game}
Here, I am
representing the probabilities as numbers from 0 to 1, which
is clearer than statements like ``The odds are 999 to 1,''
and makes the calculations easier. A probability of 0
represents something impossible, and a probability of 1
represents something that will definitely happen.  

<% marg(50) %>
<%
  fig(
    'slot-machine',
    %q{%
      The probability that one wheel will give a cherry is 1/10. The
      probability that all three wheels will give cherries is $1/10\times1/10\times1/10$.
    }
  )
%>
<% end_marg %>


This calculation was based on the following principle:

\index{independent probabilities!law of}\label{statistical-independence}
\begin{lessimportant}[the law of independent probabilities]
If the probability of one event happening is $P_A$, and the
probability of a second statistically independent event
happening is $P_B$, then the probability that they will both
occur is the product of the probabilities, $P_AP_B$.
\end{lessimportant}

This can be taken as the definition of statistical independence.\index{independence!statistical}

Note that this only applies to independent probabilities.
For example, if $P_M$ is the probability that a randomly chosen
American owns a motorcycle, and $P_K$ is the probability that a person from the same population
gets killed in a motorcycle accident, then the probability of both M and K is \emph{much}
greater than the product $P_MP_K$. This is because when K happens, it's usually \emph{because}
M was true (the exceptions being cases like people who take a ride on their friend's bike).
<% end_sec('independence') %>

<% begin_sec("Addition of probabilities",nil,'addition-of-prob') %>
\index{probabilities!addition of}

The law of independent probabilities tells us to use
multiplication to calculate the probability that both A and
B will happen, assuming the probabilities are independent.
What about the probability of an ``or'' rather than an
``and''? If two events A and $B$ are mutually exclusive,
then the probability of one or the other occurring is the
sum $P_A+P_B$. For instance, a bowler might have a 30\%
chance of getting a strike (knocking down all ten pins) and
a 20\% chance of knocking down nine of them. The bowler's
chance of knocking down either nine pins or ten pins is therefore 50\%.

It does not make sense to add probabilities of things that
are not mutually exclusive, i.e., that could both happen. Say
I have a 90\% chance of eating lunch on any given day, and a
90\% chance of eating dinner. The probability that I will
eat either lunch or dinner is not 180\%.  

<% end_sec('addition-of-prob') %>

<% begin_sec("Normalization",nil,'normalization') %>
\index{probabilities!normalization of}\index{normalization!discrete probability}
If I spin a globe and randomly pick a point on it, I have
about a 70\% chance of picking a point that's in an ocean
and a 30\% chance of picking a point on land. The probability
of picking either water or land is $70\%+30\%=100\%$. Water
and land are mutually exclusive, and there are no other
possibilities, so the probabilities had to add up to 100\%.
It works the same if there are more than two possibilities
---  if you can classify all possible outcomes into a list
of mutually exclusive results, then all the probabilities
have to add up to 1, or 100\%. This property of probabilities
is known as normalization.

<% marg(20) %>
<%
  fig(
    'globe',
    %q{%
      Normalization: the probability of picking land plus
      the probability of picking water adds up to 1.
    }
  )
%>
<% end_marg %>

<% end_sec('normalization') %>

<% begin_sec("Averages",nil,'averages') %>
\index{averages}

Another way of dealing with randomness is to take averages.
The casino knows that in the long run, the number of times
you win will approximately equal the number of times you
play multiplied by the probability of winning. In the slot-machine game
described on page \pageref{slot-machine-game}, where the probability of winning is 0.001,
if you spend a week playing, and pay \$2500 to play 10,000
times, you are likely to win about 10 times $(10,000\times0.001=10)$,
and collect \$1000. On the average, the casino will make a
profit of \$1500 from you. This is an example of the following rule.

\index{averages!rule for calculating}
\begin{lessimportant}[Rule for Calculating Averages]
If you conduct $N$ identical, statistically independent
trials, and the probability of success in each trial is $P$,
then on the average, the total number of successful trials
will be $NP$. If $N$ is large enough, the relative
error in this estimate will become small.
\end{lessimportant}

\pagebreak

<% self_check('independence',<<-'SELF_CHECK'
Which of the following things \emph{must} be independent,
which \emph{could} be independent, and which definitely are
\emph{not} independent?
(1) the probability of successfully making two free-throws
in a row in basketball; (2) the probability that it will rain in London tomorrow and
the probability that it will rain on the same day in a
certain city in a distant galaxy; 
(3) your probability of dying today and of dying tomorrow.
  SELF_CHECK
  ) %>

\startdqs

\begin{dq}
Why isn't it valid to define randomness by saying that
randomness is when all the outcomes are equally likely?
\end{dq}

<% marg(300) %>
<%
  fig(
    'dice',
    %q{Why are dice random?}
  )
%>
<% end_marg %>

\begin{dq}
Newtonian physics is an essentially perfect approximation
for describing the motion of a pair of dice. If Newtonian
physics is deterministic, why do we consider the result of
rolling dice to be random?
\end{dq}

<% end_sec('averages') %>

<% end_sec('basic-prob') %>


<% begin_sec("A statistical argument for irreversible processes",nil,'stat-irreversible') %>

We can now apply our knowledge of probability to begin to flesh out
our statistical explanation of why certain processes are irreversible.
Figure \figref{free-expansion}/1 shows a box in which all the atoms of
the gas are confined on one side. We very quickly remove the barrier
between the two sides, \figref{free-expansion}/2.  Each snapshot shows
both the positions and the momenta of the atoms, which is enough
information to allow us in theory to extrapolate the behavior of the
system into the future, or the past.  After some time, the system has
reached a state, \figref{free-expansion}/3, that seems like a typical
one that we would observe if the gas hadn't been prepared in any
special way.

<% marg(10) %>
<%
  fig(
    'free-expansion',
    %q{A gas expands freely, doubling its volume.}
  )
%>
<% end_marg %>

Let $n$ be the number of atoms in the gas. In figure \figref{free-expansion}, $n=6$.
If we pick a state of the system
completely at random, then by symmetry any atom's probability of being on
the right is $1/2$. If each atom's probability is independent of the others,\footnote{Independence is an approximation. The atoms have some size,
so there will be a tendency for them to crowd each other out. But for this to be a significant effect, we would need to consider
gases under considerable compression. In an extreme case, the gas could be
very close to the point at it would condense into a liquid or solid. Under such conditions, independence would be a very poor approximation.}
then the probability of finding \emph{all} the atoms
on the right is $2^{-n}$. When $n$ is a relatively small number like 6, this
will actually happen once in a while, as in figure \figref{fluctuation-in-free-expansion}.

Suppose
we show figure \figref{free-expansion}/2 to a friend without any further information,
and ask her what she can say about the system's behavior in the future. She doesn't
know how the system was prepared. Perhaps, she thinks, it was just a strange
coincidence that all the atoms happened to be in the right half of the box
at this particular moment. In any case, she knows that this unusual situation
won't last for long. She can predict that after the passage of any significant amount of time,
a surprise inspection is likely to show roughly
half the atoms on each side. The same is true if you ask her to say what
happened in the past. She doesn't know about the barrier, so as far as she's concerned,
extrapolation into the past is exactly the same kind of problem as extrapolation into
the future. We just have to imagine reversing all the momentum vectors, and then
all our reasoning works equally well for backwards extrapolation. She would conclude,
then, that the gas in the box underwent an unusual fluctuation, \figref{fluctuation-in-free-expansion},
and she knows that
the fluctuation is unlikely to exist very far into the future, or to have existed very
far into the past.

<% marg() %>
<%
  fig(
    'fluctuation-in-free-expansion',
    %q{%
      An unusual fluctuation in the distribution of the atoms
      between the two sides of the box. There has been no external manipulation
      as in figure \figref{free-expansion}/1.
    }
  )
%>
<% end_marg %>

Now $2^{-6}\approx0.015$ is not really all that small. But suppose we have $n=100$ atoms, which is
still a very small number. A one-liter chamber with only 100 atoms in it would still qualify as an
exceptionally good laboratory vacuum. We have $2^{-100}\approx 10^{-30}$, which is very, very small.
Our friend feels that we've played a trick on her. If she could check once every millisecond, then she
would expect to find such a fluctuation only once in about $10^{20}$ years, which is many orders of
magnitude more than her lifetime.

% calc -e "cpy=(365)(24)(3600)(1000); p=2^-100; 1/(pcpy)"
%    4.01969368413315*10^19

So when $n$ is even somewhat large, and your friend looks at the system and sees all $n$ of the atoms
on the right side of the box, she will infer that this is not a fluctuation. She can tell
that you've played a trick on her by preparing the system in a special way.

Now when we look around our universe, we see that the distribution of matter is far from uniform.
There are vast voids between the galaxies, with only about one molecule on the average in a volume
the size of a bus. There are also obviously regions with a lot more matter, such as the region you're
occupying right now. If the universe's state soon after the big bang had been chosen at random
from among all possible states, then its current state would be absurdly unlikely. We can therefore
conclude that our big bang was not a very typical one compared to all possible big bangs.
This provides our best current explanation of the ``arrow of time'' --- the past is the direction
toward the big bang.
<% end_sec('stat-irreversible') %>


<% begin_sec("Sizes of fluctuations",nil,'fluctuations') %>

In the thought experiment of the atoms bouncing around between the two sides of the box, we've
considered only two cases: the typical case, where about half the atoms are in each side,
and the extreme case where all are on one side. The real world isn't like either of these
extremes. We expect to see fluctuations, just not huge ones.
It's instructive to consider a simple example in which we can easily estimate the
the typical sizes of fluctuations. Suppose that we have three atoms in our box, and we label
them A, B, and C. Then we can have A on the left or right, which is two possibilities, and
similarly for B and C. This makes a total of $2\times2\times2=8$ possibilities, which is
not too many to list by hand. Let $r$ be the number of atoms on the right. Then we have the
eight possibilities listed in table \figref{atoms-list}.

<% marg(300) %>
<% fig('atoms-list','A list of the possible ways of putting atoms A, B, and C on the two sides of the box.',{'text'=>%q{
\begin{shaded}
\begin{tabular}{lp{20mm}p{20mm}}
$r$ & \emph{atoms on the left} & \emph{atoms on the right} \\\\
0 & A, B, C & none \\\\
1 & B, C & A \\\\
1 & A, C & B \\\\
1 & A, B & C \\\\
2 & A & B, C\\\\
2 & B & A, C \\\\
2 & C & A, B \\\\
3 & none & A, B C
\end{tabular}
\end{shaded}
},'textgap'=>-10}) %>
<% end_marg %>

Counting up all the possibilities for each value of $r$, we have the following for
the total number of possibilities $M$ at each value of $r$:

\begin{center}
\begin{tabular}{ll}
$r$ & $M$ \\
0 & 1 \\
1 & 3 \\
2 & 3 \\
3 & 1
\end{tabular}
\end{center}

\noindent Thus the probability of $r=0$ is $P(0)=1/8$, and so on.

As a check on our work, we see
that the normalization works out, $1/8+3/8+3/8+1/8=1$. Furthermore, it makes
sense that the probabilities form a symmetric pattern above and below the value $1.5$,
because our rule for calculating averages predicts that the average value of $r$
should be $3\times\frac{1}{2}$.

<% marg() %>
<%
  fig(
    'binomial',
    %q{Probability of finding $r$ out of $n=100$ balls on the right-hand side of the box. Two values that are too
       small to see are labeled with numbers.}
  )
%>
<% end_marg %>

We see that in our example, the extreme values of $r$ are considerably less likely than the
moderate values,  with probabilities that are one third as big. For larger values of $n$, this trend becomes more pronounced, and the
typical size of the fluctuations in $r$ become very small compared to $r$.

For example,
when $n=100$, we find (\note{binomial}) that the probability of
getting $r=70$ is only $2.3\times 10^{-5}$,
which is very small. Thus for $n=100$, even a 20\% fluctuation in $r$ is rare. In the graph in figure
\figref{binomial}, not only is the central peak of the bell-shaped curve fairly narrow, but its ``tails'' represent extremely
small probabilities, as shown by the labels for the invisibly small numbers. And this was only for $n=100$. For real-world
quantities of atoms, we often have $n\sim 10^{23}$, which means that fluctuations will be
immeasurably smaller still. We will estimate the sizes of these fluctuations by a different technique
in sec.~\ref{sec:variance}, p.~\pageref{two-sided-box-std-dev},
which will allow us to give a clear proof that they become negligible for large $n$, in this sense.\label{two-sided-box-simple}

<% end_sec('fluctuations') %>

<% begin_sec("Equipartition",nil,'equipartition') %>\label{equipartition}
Betsy Salazar of Redwood Cove, California, has 37 pet raccoons, which
is theoretically illegal. She admits that she has trouble telling them
apart, but she tries to give them all plenty of care and affection
(which they reciprocate). There are only so many hours in a day, so
there is a fixed total amount of love. The raccoons share this love
unequally on any given day, but \emph{on the average} they all get the
same amount. This kind of
equal-sharing-on-the-average-out-of-some-total-amount is more
concisely described using the term \emph{equipartition}, meaning equal
partitioning, or equal sharing. If Betsy did keep track of how much
love she lavished on each animal, using some numerical scale, we would
have 37 numbers to keep track of. We say that the love is partitioned
among 37 \emph{degrees of freedom}.

By analogy with the raccoons, we've seen that physical systems don't like to
put all their eggs in one basket. In our example of the atoms in a box, it
is overwhelmingly likely, for real-world quantities of atoms, that the atoms
will be shared almost equally between the two sides.

We haven't yet discussed the energy of the atoms, but it's in connection
with energy that we normally use the term equipartition. Consider a system
consisting of only two atoms, and let the atoms, like the raccoons, differ in size.
One has mass $M$ and other other $m$. For simplicity, we take the system to be
one-dimensional. Then the total kinetic energy of the two atoms is
$  \frac{1}{2}MV^2+\frac{1}{2}mv^2$,
where $V$ is the velocity of the big atom and $v$ the velocity of the little one.
In thermodynamics we actually usually prefer to discuss momenta rather than velocities (\note{p-not-v}),
so using the identity $K=p^2/2m$, we have for the energy
\begin{equation*}
  \frac{P^2}{2M}+\frac{p^2}{2m}.
\end{equation*}
This total energy is conserved, but it can be shared (``partitioned'') in various ways
between the two atoms. It can be proved that, on the average, each atom has an equal
share of the energy, regardless of the unequal masses. 

If we consider all three dimensions of space, then equipartition says that on the average,
an equal amount of energy goes to each dimension. For example, a particular atom will
have some amount of kinetic energy in the form of its $x$ motion, and on the average
this will be the \emph{same} as the amount in its $y$ and $z$ motions. In fancy
language, each of these three ``degrees of freedom'' gets an equal share of energy on the
average.\index{degree of freedom}\label{degree-of-freedom}

Equipartition holds even if the energy
isn't kinetic energy or if the system isn't a gas. For example, in a solid, the atoms
are stuck near certain positions, and they can't move freely. They can only oscillate about
these positions, like masses on little springs. The potential energy of a spring is
$(1/2)kx^2$, and because this is a constant multiplied by the square of something,
equipartition still holds.

Heat is the random motion of atoms. In a hot substance, the average energy of an atom
will be higher. Equipartition gives us a natural way to define temperature,
as something like the average energy per degree of freedom. Of course, this
number comes out to be inconveniently small in SI units, and in any case there was
a preexisting system of temperature units, so we define a scale of temperature $T$ with
a fudge factor, notated $k/2$, such that equipartition comes out like this:

\begin{important}[Equipartition]
Suppose that a system has an energy that can be expressed in the way described above (as a sum
of terms that look like the squares of $x$'s and/or $p$'s), and the system is in equilibrium.
Then each of these
degrees of freedom has, on average, an energy equal to $\frac{1}{2}kT$.
\end{important}

\noindent The constant $k$ is called the Boltzmann constant, and in SI units it equals
$1.38\times10^{-23}\ \junit/\zu{K}$. Here K stands for the kelvin, a unit of temperature.
One degree kelvin is the same temperature difference as one degree celsius, but the zero
of the kelvin scale is absolute zero, the coldest possible temperature, at which all
molecular motion ceases.\index{temperature!microscopic definition}
\index{temperature!celsius}\index{temperature!kelvin}\index{temperature!absolute zero}
\index{Celsius (unit)}\index{kelvin (unit)}\label{kelvin}

Equipartition gives us a straightforward way to estimate the \emph{heat capacities}
of various substances, meaning how much energy it takes to raise their temperature by
a certain amount.

\begin{eg}{Heating helium}\label{eg:heating-helium}
Helium is a monatomic gas, so it has three degrees of freedom: motion in the $x$, $y$, and $z$
directions. (Diatomic and polyatomic gases have more degrees of freedom, because they can also
rotate, and possibly vibrate.) A liter of helium contains $n=2.5\times10^{22}$ atoms. Each atom
has an average kinetic energy of $(3/2)kT$, for a total of $(3/2)nkT$. The amount of heat required
to raise its temperature by one degree is
\begin{equation*}
  \frac{3}{2}nk(1\ \zu{K})=0.51\ \junit.
\end{equation*}
This is the heat capacity at constant volume for this volume of gas. Many other, related quantities
can be defined. One can, for example, define the heat capacity per unit mass, the capacity per mole, or
the capacity at constant pressure (meaning that the gas must be allowed to expand, and therefore
use up some of the input energy by doing work on the walls of the container).
\end{eg}

% calc -x -e "T=293; V=10^-3 m3; P=10^5 N/m2; PV/(kBT)"
%    2.47200179862853*10^22

<% marg() %>
<%
  fig(
    'dulong-petit',
    %q{%
      Heat capacities of solids cluster around $3kT$ per atom. The notation $\bar{E}$ means the average energy.
      The elemental solids plotted are the ones originally
      used by Dulong and Petit to infer empirically that the heat capacity of solids per atom was constant.
      (Modern data.)\index{Dulong-Petit law}
    }
  )
%>
<% end_marg %>

\begin{eg}{Heating solids}\label{eg:dulong-petit}
For a solid, we expect there to be a total of \emph{six}
energies per atom: the three kinetic energies, plus potential energies due to its displacement in the $x$, $y$, and $z$ directions.
Each of these carries an average energy $\frac{1}{2}kT$, for a total
of $3kT$. In other words, a solid should have twice the heat capacity found in example \ref{eg:heating-helium}
for a monatomic gas. This was discovered empirically by Dulong and Petit
in 1819, as shown in figure \figref{dulong-petit}.\index{specific heat!solids}
\end{eg}

<% end_sec('equipartition') %>

<% begin_sec("Heat capacities of gases",nil,'heat-capacity-gases') %>
Examples \ref{eg:heating-helium} and \ref{eg:dulong-petit} show that by measuring the heat capacity of a substance, we can
find out how many microscopic degrees of freedom it has. As a surprising example of how far this can carry us,
consider the three types of molecules shown in figure \figref{molecular-shapes}.

<%
  fig(
    'molecular-shapes',
    %q{% 
         The differing shapes of a helium atom (1), a nitrogen molecule (2), and a difluoroethane molecule (3) have surprising
         macroscopic effects.
    },
    {
      'width'=>'wide',
      'sidecaption'=>true
    }
  )
%>

If the gas is monatomic, then we know from the result of example \ref{eg:heating-helium} that 
the relationship between its temperature and its
internal energy is $E=(3/2)nkT$, so that to heat it by one degree, we the amount of heat we need to
add\footnote{This is called the specific heat capacity, or specific heat, per atom. In practical work it is
more common to multiply by Avogadro's number, which gives the specific heat per mole.} is $(3/2)nk$.
Here the factor of 3 came ultimately from the fact that the gas was in a
three-dimensional space, \subfigref{molecular-shapes}{1}. Moving in this space, each molecule can have momentum in the x, y, and
z directions. It has three degrees of freedom.

What if the gas is not monatomic? Air, for example, is made of diatomic molecules,
\subfigref{molecular-shapes}{2}. There is a subtle difference between the two cases. An individual atom of
a monatomic gas is a perfect sphere, so it is exactly the same no matter how it is oriented. Because of this perfect symmetry, there is
thus no way to tell whether it is spinning or not, and in fact we find that it can't rotate.\footnote{Later in this
course, we'll see in more detail that this is a consequence of how quantum physics describes motion}
The diatomic gas, on the other hand,
can rotate end over end about the $x$ or $y$ axis, but cannot rotate about the $z$ axis, which is its axis of symmetry.
It has a total of five degrees of freedom. A polyatomic molecule with a more complicated, asymmetric shape,
\subfigref{molecular-shapes}{3}, can rotate about all three axis, so it has a total of six degrees of freedom.
Summarizing, we have
\begin{equation*}
  \alpha = \begin{cases}
    3, \text{monatomic gas} \\
    5, \text{diatomic gas} \\
    6, \text{polyatomic gas},
  \end{cases}
\end{equation*}
for the number of degrees of freedom per molecule $\alpha$, and the specific heat per molecule, at constant volume, is $(\alpha/2)nk$.


In 1819, Cl\'{e}ment and Desormes did a simple experiment that we
would today interpret as measuring this heat capacity for air. Its
correct interpretation, in comparison with other gases, requires
talking about the shapes of molecules. This was in a time when the
periodic table hadn't been invented, and the existence of atoms and
molecules was considered a mere hypothesis.  Who would have dreamed
that such simple observations, correctly interpreted, could give us
this kind of glimpse of the microcosm?

<% end_sec('heat-capacity-gases') %>

<% begin_sec("The ideal gas law",nil,'ideal-gas') %>
Suppose that we take a liter of helium gas and get it very hot, increasing its absolute temperature by
a factor of 4. This increases the average kinetic energy per atom by a factor of 4, and since
kinetic energy depends on the square of the velocity, the typical speed is twice as big as before.
This gas exerts a pressure on the sides of its container. The pressure is the force per unit area,
measured in SI units of $\nunit/\munit^2$, which can be abbreviated as pascals, $1\ \zu{Pa}=1 \nunit/\munit^2$.
For two different reasons, the doubling of the speeds of the atoms will now lead to an increase in the pressure.
First, the frequency of collisions with the wall has been doubled. Second, the typical momentum of each atom has
also been doubled. Since force is the rate of transfer of momentum, $F=\Delta p/\Delta t$, this means that
the force is quadrupled. Our conclusion is that when we heat the gas by a factor of 4, while keeping
the volume constant, the pressure goes up by a factor of 4. That is, the pressure of an ideal gas is
proportional to its temperature. (By an ideal gas, we mean one whose atoms take up negligible space and
do not interact with one another, e.g., by condensing to form droplets.) A straightforward extension of
these arguments leads to the following law.

\begin{important}[Ideal gas law]\index{ideal gas law}
For an ideal gas,
\begin{equation*}
        PV        =  nkT,
\end{equation*}
where $P$ is the pressure, $V$ is the volume, $n$ is the number of molecules, and $T$ is the absolute pressure.
\end{important}

\noindent (You may have seen this
written elsewhere as $PV=NRT$, where $N=n/N_A$ is the number of
moles of atoms, $R=kN_A$, and $N_A=6.0\times10^{23}$, called
Avogadro's number, is essentially the number of hydrogen
atoms in 1 g of hydrogen.)\index{Avogadro's number}

\pagebreak

\begin{eg}{Pressure in a car tire}
\egquestion
After driving on the freeway for a while, the air
in your car's tires heats up from $10\degcunit$ to $35\degcunit$. How much
does the pressure increase?

\eganswer
The tires may expand a little, but we assume this
effect is small, so the volume is nearly constant. From the
ideal gas law, the ratio of the pressures is the same as the
ratio of the absolute temperatures,
\begin{align*}
          P_2/ P_1
                &= T_2/ T_1\\
                &=(308\ \kunit)/(283\ \kunit)\\
                &= 1.09\eqquad,\\
\end{align*}
or a 9\% increase.
\end{eg}

<% end_sec('ideal-gas') %>

<% begin_notes %>

\notetext{binomial}{Calculating probabilities for the two-sided box when $n$ is large}
\notesummary{We demonstrate techniques that make it possible to calculate these probabilities for large $n$.}
In the text, we explicitly listed all the possibilities for distributing $n=3$ atoms between the two sides
of the box. This becomes impractical for large $n$. A mathematical technique that is more practical involves
the use of numbers called binomial coefficients, which you may have encountered before in the context of algebra. In the case of $n=3$,
consider what happens when we calculate $(1+x)^3=1+3x+3x^2+x^3$. The set of numbers 1, 3, 3, 1 is the same one we computed
by listing possibilities for our atoms. The idea is that when we multiply out the cube, we have to go through all the combinations
of 1's and $x$'s in the three factors. This is why these numbers are called binomial coefficients.\index{binomial coefficient}

The binomial coefficient ${n \choose r}$ is defined as the number of ways of choosing $r$ things from among $n$ objects, with
the order of the choices not being significant.
For example, ${3 \choose 2}=3$, because we can choose A and B, B and C, or A and C. (But we don't consider ``A and C'' to be
a different choice than ``C and A.'')

A binomial coefficient can be computed in terms of the factorial function defined by $m!=1\times2\times\ldots m$,
as ${n \choose r}=n!/r!(n-r)!$. This works because we have $n$ choices for the first object, $n-1$ for the second, and so on, down to
$n-r+1$ choices for the final one. The product of these factors is the same as $n!/(n-r)!$. But this number counts possibilities more
than once if they occur in a different order. Each possibility will be counted $r!$ times. Therefore we divide by $r!$ to get rid of
the overcounting. For the example in the text, we have
\begin{equation*}
  {100 \choose 70} = 29372339821610944823963760,
\end{equation*}
which produces the probability
\begin{equation*}
  \frac{29372339821610944823963760}{2^{100}} = 2.3\times 10^{-5}.
\end{equation*}

Calculators and computers will often spit out overflow errors when we ask them to calculate large binomial coefficients
like this one. A good way of dealing with this problem is to work with the logarithms of the numbers, and to
employ the approximation $\ln n!\approx n\ln n-n$, known as Stirling's formula, which comes from approximating the sum
$\sum_{j=1}^n \ln j$ as $\int_1^n \ln x \der x$.\index{Stirling's formula}

\notetext{p-not-v}{Why we use $p$ rather than $v$ for our statistics}
To describe the state of a system of particles (in one dimension, for simplicity), we could
give either the list of numbers $(x_1,v_1,x_2,v_2,\ldots)$ or the list
$(x_1,p_1,x_2,p_2,\ldots)$ If the particles all have the same mass, then it makes no
difference which description we use. But if the particles have different masses, then
the latter description is preferable, for the following reason. Suppose we make a graph-paper
grid with $x_1$ on one axis and $p_1$ on the other. Then a mathematical result called
Liouville's theorem shows that a reasonable initial guess about
probabilities is to assign equal probability to each square on the grid.

Although the full statement of Liouville's theorem is beyond the scope
of this book, it's not hard to see what goes wrong if we try to use
velocities instead. When object of different mass collide, they cannot
transfer energy in an arbitrary way while still obeying conservation
of momentum.  For example, if you throw a golf ball with an energy of
10 joules, and the golf ball hits a bowling ball, it is not possible
for the bowling ball to absorb all 10 joules of energy from the golf
ball in the form of kinetic energy. The result is that statistically,
in such collisions, there is a tendency for the less massive object to
undergo bigger accelerations and have larger velocities.  This means
that it is not reasonable to assign the same probability per unit
\emph{velocity} to the golf ball as to the bowling ball.

<% end_notes %>


<% begin_hw_sec(vfill:true) %>

<% begin_hw('stpvol',0) %>__incl(hw/stpvol)<% end_hw() %>

<% begin_hw('work-pdv') %>__incl(hw/work-pdv)<% end_hw() %>

<% begin_hw('cracking-gas-molecules') %>__incl(hw/cracking-gas-molecules)<% end_hw() %>

<% begin_hw('sun-heating',0) %>__incl(hw/sun-heating)<% end_hw() %>

<% begin_hw('igm') %>__incl(hw/igm)<% end_hw() %>

<% begin_hw('fusion') %>__incl(hw/fusion)<% end_hw() %>

<% begin_hw('electron-speed-in-metal') %>__incl(hw/electron-speed-in-metal)<% end_hw() %>

<% end_hw_sec %>

<% end_chapter() %>
