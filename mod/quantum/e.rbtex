<%
  require "../eruby_util.rb"
%>

<%
  chapter(
    '17',
    %q{The Schr\\"odinger equation},
    'ch:schrodinger'
  )
%>

<% begin_sec("Electrons in electric fields",nil,'electrons-in-fields') %>
<% begin_sec("Defining a wavelength when the wavelength is varying",nil,'varying-lambda') %>

So far the only electron wave patterns we've considered have
been simple sine waves, but whenever an electron finds
itself in an electric field, it must have a more complicated
wave pattern. Let's consider the example of an electron
being accelerated by the electron gun at the back of a TV
tube. Newton's laws are not useful, because they implicitly
assume that the path taken by the particle is a meaningful
concept. Conservation of energy is still valid in quantum
physics, however. In terms of energy, the electron is moving
from a region of low voltage into a region of higher
voltage. Since its charge is negative, it loses electrical energy by moving
to a higher voltage, so its kinetic energy increases. As its electrical
energy goes down, its kinetic energy goes up by an equal
amount, keeping the total energy constant. Increasing
kinetic energy implies a growing momentum, and therefore a
shortening wavelength, \figref{accelerating-electron}.

The wavefunction as a whole does not have a single
well-defined wavelength, but the wave changes so gradually
that if you only look at a small part of it you can still
pick out a wavelength and relate it to the momentum and
energy. (The picture actually exaggerates by many orders of
magnitude the rate at which the wavelength changes.)

<% marg(100) %>
<%
  fig(
    'accelerating-electron',
    %q{%
      An electron in a gentle electric 
      field gradually shortens its wavelength as it gains energy.
      (As discussed on p.~\pageref{cheat-with-real-psi}, it is actually not quite correct to
      graph the wavefunction of an electron as a real number unless it is a standing wave, which
      isn't the case here.)
    }
  )
%>
<% end_marg %>



But what if the electric field was stronger? The electric
field in an old-fashioned vacuum tube TV screen is only $\sim10^5$  N/C, but the electric field
within an atom is more like $10^{12}$  N/C. In figure \figref{osculating},
the wavelength changes so rapidly that there is nothing that
looks like a sine wave at all. We could get a rough idea of
the wavelength in a given region by measuring the distance
between two peaks, but that would only be a rough approximation.
Suppose we want to know the wavelength at point $P$. The
trick is to construct a sine wave, like the one shown with
the dashed line, which matches the curvature of the actual
wavefunction as closely as possible near $P$. The sine wave
that matches as well as possible is called the ``osculating''
curve, from a Latin word meaning ``to kiss.'' The wavelength
of the osculating curve is the wavelength that will relate
correctly to conservation of energy.

<%
  fig(
    'osculating',
    %q{%
      A typical wavefunction of
       an electron in an atom (heavy curve) and the
       osculating sine wave (dashed curve) that matches its curvature at point P.
    },
    {
      'width'=>'wide'
    }
  )
%>

\index{tunneling}
<% end_sec('varying-lambda') %>
<% begin_sec("Tunneling",nil,'tunneling-simple') %>

<% marg() %>
<%
  fig(
    'kinks',
    %q{%
      The wavefunction's tails go where classical
      physics says they shouldn't.
    }
  )
%>
<% end_marg %>

We implicitly assumed that the particle-in-a-box wavefunction
would cut off abruptly at the sides of the box, \figref{kinks}/1, but
that would be unphysical. A kink has infinite curvature, and
curvature is related to energy, so it can't be infinite. A
physically realistic wavefunction must always ``tail off''
gradually, \figref{kinks}/2. In classical physics, a particle can never
enter a region in which its interaction energy $U$ would be
greater than the amount of energy it has available. But in
quantum physics the wavefunction will always have a tail
that reaches into the classically forbidden region. If it
was not for this effect, called tunneling, the fusion
reactions that power the sun would not occur due to the high
electrical energy nuclei need in order to get close together!
Tunneling is discussed in more detail on p.~\pageref{quantitativetunneling}.

<% end_sec('tunneling-simple') %>

<% end_sec('electrons-in-fields') %>

<% begin_sec("The Schr\\\"odinger equation",nil,'schrodinger') %>

\index{Schr\"odinger equation}
In section \ref{sec:electrons-in-fields} we were able to apply conservation
of energy to an electron's wavefunction, but only by using
the clumsy graphical technique of osculating sine waves as a
measure of the wave's curvature. You have learned a more
convenient measure of curvature in calculus: the second
derivative. To relate the two approaches, we take the second
derivative of a sine wave:
\begin{align*}
 \frac{\der^2}{\der x^2}\sin kx
 &= \frac{\der}{\der x}\left(k\cos kx\right)     \\
         &= -k^2 \sin kx.
\end{align*}

Taking the second derivative gives us back the same
function, but with a minus sign and a constant out in front
that is related to the wavelength. We can thus relate the
second derivative to the osculating wavelength:
\begin{equation}\label{eq:schreqna}
        \frac{\der^2\Psi}{\der x^2} = -k^2\Psi,
\end{equation}
where $k=2\pi/\lambda$.
This could be solved for $k$ or $\lambda $ in terms of $\Psi $, but
it will turn out below to be more convenient to leave it in this form.

Applying this to conservation of energy, we have
\begin{align}\label{eq:schreqnb}
\begin{split}
           E         &=    K  +  U  \\
                 &=   \frac{p^2}{2m}  + U  \\
                 &=   \frac{(\hbar k)^2}{2m}  + U        
\end{split}
\end{align}

\noindent We can simplify our algebra by
multiplying both sides of equation \eqref{eq:schreqnb} by $\Psi $ to make it
look more like equation \eqref{eq:schreqna}:
\begin{equation*}
        E \cdot \Psi          =    \frac{(\hbar k)^2}{2m}\Psi    +   U \cdot \Psi  ,
\end{equation*}
which leads to the important result known as
the \labelimportantintext{Schr\"odinger equation}:

\begin{important}\label{s-eqn-simplest-initial-statement}
\begin{equation*}
        E \cdot \Psi = -\frac{\hbar^2}{2m}\frac{\der^2\Psi}{\der x^2}  +   U \cdot \Psi
\end{equation*}
\end{important}
\noindent (Actually this is a simplified version of the Schr\"odinger
equation, applying only to standing waves in one dimension.)
Physically it is a statement of conservation of energy. The
total energy $E$ must be constant, so the equation tells us
that a change in interaction energy $U$ must be accompanied by a
change in the curvature of the wavefunction. This change in
curvature relates to a change in wavelength, which
corresponds to a change in momentum and kinetic energy.

<% self_check('schrodingerassumptions',<<-'SELF_CHECK'
Considering the assumptions that were made in deriving the
Schr\"odinger equation, would it be correct to apply it to a
photon? To an electron moving at relativistic speeds?
  SELF_CHECK
  ) %>

Usually we know right off the bat how $U$ depends on
$x$, so the basic mathematical problem of quantum physics is
to find a function $\Psi (x$) that satisfies the Schr\"odinger
equation for a given interaction-energy function $U(x)$.
An equation, such as the Schr\"odinger equation, that
specifies a relationship between a function and its
derivatives is known as a differential equation.

\pagebreak

\startdq

\begin{dq}
The figure shows a skateboarder tipping over into a swimming pool with zero
initial kinetic energy. There is no friction, the corners are smooth enough to allow the
skater to pass over the smoothly, and the vertical distances are small
enough so that negligible time is required for the vertical parts of the motion.
The pool is divided into a deep end and a shallow end. Their widths are equal.
The deep end is four times deeper. (1) Classically, compare the skater's velocity in
the left and right regions, and infer the probability of finding the skater in either of the
two halves if an observer peeks at a random moment.
(2) Quantum-mechanically, this could be a one-dimensional model of an electron shared between two atoms
in a diatomic molecule. Compare the electron's kinetic energies, momenta, and wavelengths in the
two sides. For simplicity, let's assume that there is no tunneling into the classically forbidden
regions. What is the simplest standing-wave pattern that you can draw, and what are the probabilities
of finding the electron in one side or the other? Does this obey the correspondence principle?
\end{dq}

<%
  fig(
    'quantum-pool-skater',
    '',
    {                                 
      'width'=>'fullpage'
    }
  )
%>

<% end_sec('schrodinger') %>

<% begin_sec("Solutions when $U$ is constant; tunneling",nil,'sch-const-u-solutions') %>

The detailed study of the solution of the Schr\"odinger equation is
beyond the scope of this book,
but we can gain some
important insights by considering the easiest version of the
Schr\"odinger equation, in which the interaction energy $U$ is
constant. We can then rearrange the Schr\"odinger equation as follows:
\begin{align*}
   \frac{\der^2\Psi}{\der x^2} &= \frac{2m(U-E)}{\hbar^2} \Psi\eqquad,
\intertext{which boils down to}
   \frac{\der^2\Psi}{\der x^2} &= a\Psi\eqquad,
\end{align*}
where, according to our assumptions, $a$ is independent of
$x$. We need to find a function whose second derivative is
the same as the original function except for a multiplicative
constant. The only functions with this property are sine
waves and exponentials:

\begin{align*}
 \frac{\der^2}{\der x^2}\left[\:q\sin(rx+s)\:\right] &= -qr^2\sin(rx+s)     \\
 \frac{\der^2}{\der x^2}\left[qe^{rx+s}\right] &= qr^2e^{rx+s}
\end{align*}

The sine wave gives negative values of $a$, $a=-r^2$, and the
exponential gives positive ones, $a=r^2$. The former applies
to the classically allowed region with $U<E$.

\label{quantitativetunneling}

<% marg(60) %>
<%
  fig(
    'barrier-with-u-notation',
    %q{Tunneling through a barrier.
      (As discussed on p.~\pageref{cheat-with-real-psi}, it is actually not quite correct to
      graph the wavefunction of an electron as a real number unless it is a standing wave, which
      isn't the case here.)
    }
  )
%>
<% end_marg %>

This leads us to a quantitative calculation of the tunneling
effect discussed briefly in sec.~\ref{subsec:tunneling-simple}, p.~\pageref{subsec:tunneling-simple}. The
wavefunction evidently tails off exponentially in the
classically forbidden region. Suppose, as shown in
figure \figref{barrier-with-u-notation},
a wave-particle traveling to the right encounters a
barrier that it is classically forbidden to enter. Although
the form of the Schr\"odinger equation we're using technically
does not apply to traveling waves (because it makes no
reference to time), it turns out that we can still use it to
make a reasonable calculation of the probability that the
particle will make it through the barrier. If we let the
barrier's width be $w$, then the ratio of the wavefunction
on the left side of the barrier to the wavefunction on the right is
\begin{equation*}
        \frac{qe^{rx+s}}{qe^{r(x+w)+s}}  = e^{-rw}\eqquad.  
\end{equation*}
Probabilities are proportional to the squares of wavefunctions,
so the probability of making it through the barrier is\label{tunneling-probability}
\begin{align*}
   P         &=  e^{-2rw}    \\
         &= \exp\left(-\frac{2w}{\hbar}\sqrt{2m(U-E)}\right) .
\end{align*}

<% self_check('walkthroughwall',<<-'SELF_CHECK'
If we were to apply this equation to find the probability
that a person can walk through a wall, what would the small
value of Planck's constant imply?
  SELF_CHECK
  ) %>

\begin{eg}{Tunneling in alpha decay}\label{eg:alpha-tunneling}
Naively, we would expect alpha decay to be a very fast process. The typical speeds of neutrons and
protons inside a nucleus are extremely high (see problem \ref{hw:lead}). If we imagine an alpha
particle coalescing out of neutrons and protons inside the nucleus, then at the typical speeds we're
talking about, it takes a ridiculously small amount of time for them to reach the surface and try
to escape. Clattering back and forth inside the nucleus, we could imagine them making a vast number of
these ``escape attempts'' every second.

Consider 
figure \figref{alpha-potential}, however, which shows the interaction energy for an alpha particle escaping from a nucleus.
The electrical energy is $kq_1q_2/r$ when the alpha is outside the nucleus, while its variation inside
the nucleus has the shape of a parabola, as a consequence of the shell theorem.
The nuclear energy is constant when the alpha is inside the nucleus, because the forces from all the
neighboring neutrons and protons cancel out; it rises sharply near the surface, and flattens out
to zero over a distance of $\sim 1$ fm, which is the maximum distance scale at which the strong force
can operate.
There is a classically forbidden region immediately outside the nucleus, so the alpha particle can only
escape by quantum mechanical tunneling. (It's true, but somewhat counterintuitive, that a \emph{repulsive}
electrical force can make it more difficult for the alpha to get \emph{out}.) 

<% marg() %>
<%
  fig(
    'alpha-potential',
    %q{The electrical, nuclear, and total interaction energies for an alpha particle escaping from a nucleus.}
  )
%>
<% end_marg %>

In reality, alpha-decay half-lives are often extremely long --- sometimes billions of years --- because
the tunneling probability is so small. Although the shape of the barrier is not a rectangle, the equation
for the tunneling probability on page \pageref{tunneling-probability}
can still be used as a rough guide to our thinking. Essentially the tunneling probability is so small
because $U-E$ is fairly big, typically about 30 MeV at the peak of the barrier.
\end{eg}

\begin{eg}{A marble tunneling out of a box}\label{eg:marble-tunneling}
On p.~\pageref{totalitarian}, I introduced Gell-Mann's whimsically named totalitarian principle, that
any process not forbidden by a conservation law will happen with some nonzero probability or rate.
One of the examples I used there was a marble locked in a box. As a silly example, let's make
a crude numerical estimate of the marble's probability of tunneling out through the box. As in self-check
\ref{sc:walkthroughwall} on p.~\pageref{sc:walkthroughwall}, we expect based on the correspondence
principle that this probability will be very small.

A typical marble has a radius on the order of a centimeter, and let's say the box has about
that thickness. We'll say that the marble has a mass of 5 g and contains something like $n=10^{23}$ atoms. The atomic
energy scale is roughly 1 eV, so that
if the marble is to be \emph{inside} the wall of the box, its electrical energy $U$ probably has to be something
on the order of $n$ multiplied by 1 eV, giving $U\sim10^4\ \junit$, or about an order of magnitude
greater than the kinetic energy of a bullet. We could of course give the marble that much energy,
and then it would blast through the box easily, but the point here is to estimate its probability of
just tunneling out, so we'll say that the kinetic energy is negligible, and therefore $U-E$ is basically
the same as $U$. Plugging in numbers, we find $P\sim e^{-10^{33}}$. When you have to express a small number
using a stack of exponents like this, you know that it's very small. For example, this number is many, many
orders of magnitude smaller than $10^{-1000}$.
\end{eg}



\begin{eg}{Beta decay: a push or pull on the way out the door}
The nucleus ${}^{64}\text{Cu}$ undergoes $\beta^+$ and $\beta^-$ decay with similar probabilities
and energies. Each of these decays releases a fixed amount of energy $Q$ due to 
the difference in mass between the parent nucleus and the decay products.
This energy is shared randomly between the beta and the neutrino. In experiments,
the beta's energy is easily measured, while the neutrino flies off without interacting.
Figure \figref{cu64-betas}
shows the energy spectrum of the $\beta^+$ and $\beta^-$ in these decays.\footnote{Redrawn
from Cook and Langer, 1948.} There is a relatively
high probability for the beta and neutrino each to carry off roughly half the kinetic energy,
the reason being identical to the kind of phase-space argument discussed in sec.~\ref{sec:phase-space},
p.~\pageref{sec:phase-space}. Therefore in each case we get a bell-shaped curve stretching from
0 up to the energy $Q$, with $Q$ being slightly different in the two cases.

<% marg(10) %>
<%
  fig(
    'cu64-betas',
    %q{$\beta^+$ and $\beta^-$ spectra of ${}^{64}\text{Cu}$.}
  )
%>
<% end_marg %>

So we expect the two bell curves to
look almost the same except for a slight rescaling of the horizontal
axis. Yes --- but we also see markedly
different behavior at low energies. At very low energies, there is almost
no chance to see a $\beta^+$ with very low energy, but
quite a high probability for a $\beta^-$.

We could try to explain this difference in terms of the release of
electrical energy.  The $\beta^+$ is repelled by the
nucleus, so it gets an extra push on the way out the door.  A
$\beta^-$ should be held back as it exits, and so should lose some
energy. The bell curves should be shifted up
and down in energy relative to one another, as observed.

But if we try to estimate this energy shift using classical
physics, we come out with a wildly incorrect answer. This would be
a process in which the beta and neutrino are
released in a pointlike event inside the nucleus. 
The radius $r$ of the ${}^{64}\text{Cu}$ nucleus is on the order of 4 fm 
($1\ \zu{fm}=10^{-15}\ \munit$). Therefore the  energy lost or gained by the
$\beta^+$ or $\beta^-$ on the way out would be $U\sim kZe^2/r\sim10\
\text{MeV}$. The actual shift is much smaller.

To understand what's really going on, we need quantum mechanics. A beta in
the observed energy range has a
wavelength of about 2000 fm, which is hundreds of times
greater than the size of the nucleus. Therefore the beta cannot be
much better localized than that when it is emitted.  This means that
we should really use something more like $r\sim 500\ \text{fm}$ (a
quarter of a wavelength) in our calculation of the electrical energy.
This gives $U\sim0.08\ \text{MeV}$, which is about the right order of
magnitude compared to observation. 

A byproduct of this analysis is that a $\beta^+$ is always emitted
within the classically forbidden region, and then has to tunnel out
through the barrier. As in example \ref{eg:alpha-tunneling}, we have
the counterintuitive fact about quantum
mechanics that a repulsive force can \emph{hinder} the escape of a
particle.
\end{eg}
<% end_sec('sch-const-u-solutions') %>

<% begin_sec("Three dimensions",nil,'sch-three-d') %>

For simplicity, we've been considering the Schr\"odinger equation in one dimension,
so that $\Psi$ is only a function of $x$, and has units of $\munit^{-1/2}$ rather
than  $\munit^{-3/2}$.
Since the Schr\"odinger equation is a statement of conservation of energy, and
energy is a scalar, the generalization to three dimensions isn't particularly
complicated. The total energy term $E\cdot\Psi$ and the interaction energy term
$U\cdot\Psi$ involve nothing but scalars, and don't need to be changed at all.
In the kinetic energy term, however, we're essentially basing our computation of
the kinetic energy on the squared magnitude of the momentum, $p_x^2$,
and in three dimensions this would clearly have to be generalized to
$p_x^2+p_y^2+p_z^2$. The obvious way to achieve this is to replace
the second derivative $\der^2\Psi/\der x^2$ with the sum
 $\partial^2\Psi/\partial x^2+ \partial^2\Psi/\partial y^2+ \partial^2\Psi/\partial z^2$. 
In other words, we replace the second derivative with the Laplacian (sec.~\ref{subsec:laplacian}, p.~\pageref{subsec:laplacian}),
\begin{equation*}
  \nabla^2 = \frac{\partial^2}{\partial x^2}+\frac{\partial^2}{\partial y^2}+\frac{\partial^2}{\partial z^2},
\end{equation*}
just as we did for waves such as sound.
We recall from sec.~\ref{subsec:laplacian} that:
\begin{itemize}
\item The partial derivative symbol $\partial$, introduced on page \pageref{partial-der}, indicates that
  when differentiating with respect to a particular variable, the other variables are to be considered
  as constants. 
\item Like the second derivative, the Laplacian is essentially a measure of curvature.
\item As shown in figure \figref{laplacian-geometrical-mod}, p.~\pageref{fig:laplacian-geometrical-mod}, we can also
   think of the Laplacian as a measure of how much the value
   of a function at a certain point differs from the average of its value on nearby points.
\end{itemize}

\begin{eg}{A classically allowed region with constant $U$}
In a classically allowed region with constant $U$, we expect the solutions
to the Schr\"odinger equation to be sine waves. A sine wave in three dimensions
has the form
\begin{equation*}
  \Psi = \sin\left( k_x x + k_y y + k_z z  \right)\eqquad.
\end{equation*}
When we compute $\partial^2\Psi/\partial x^2$, double differentiation of
$\sin$ gives $-\sin$, and the chain rule brings out a factor of $k_x^2$.
Applying all three second derivative operators, we get
\begin{align*}
  \nabla^2\Psi &= \left(-k_x^2-k_y^2-k_z^2\right)\sin\left( k_x x + k_y y + k_z z  \right) \\
               &=  -\left(k_x^2+k_y^2+k_z^2\right)\Psi\eqquad.
\end{align*}
The Schr\"odinger equation gives
\begin{align*}
  E\cdot\Psi &= -\frac{\hbar^2}{2m}\nabla^2\Psi + U\cdot\Psi \\
             &= -\frac{\hbar^2}{2m}\cdot -\left(k_x^2+k_y^2+k_z^2\right)\Psi + U\cdot\Psi \\
  E-U        &= \frac{\hbar^2}{2m}\left(k_x^2+k_y^2+k_z^2\right)\eqquad,
\end{align*}
which can be satisfied since we're in a classically allowed region with $E-U>0$, and the right-hand
side is manifestly positive.
\end{eg}

<% marg(-300) %>
<%
  fig(
    'hydrogen-ground-state-with-scale',
    %q{%
      The ground-state wavefunction of the hydrogen atom, example \ref{eg:h-ground-exact}, is graphed
      in the $x$-$y$ plane as $\Psi(x,y,0)$, omitting the third spatial dimension.
    }
  )
%>
<% end_marg %>

\begin{eg}{Exact treatment of the ground state of hydrogen}\label{eg:h-ground-exact}
The general treatment of the hydrogen atom for all values of $n$ is beyond the mathematical
scope of this book, but it's fairly straightforward to verify it for a particular $n$,
especially given a lucky guess as to what functional form to try for the wavefunction.
The form that works for the ground state is
\begin{equation*}
  \Psi = ue^{-r/a}\eqquad,
\end{equation*}
where $r=\sqrt{x^2+y^2+z^2}$ is the electron's distance from the proton, and $u$ provides for normalization.
We showed in example \ref{eg:laplacian-h-ground}, p.~\pageref{eg:laplacian-h-ground} that the Laplacian of
this function is
\begin{equation*}
  \nabla^2\Psi = \left( -\frac{2}{ar} + \frac{1}{a^2} \right) \Psi\eqquad.
\end{equation*}
The Schr\"odinger equation gives
\begin{align*}
  E\cdot\Psi &= -\frac{\hbar^2}{2m}\nabla^2\Psi + U\cdot\Psi \\
             &= \frac{\hbar^2}{2m}\left( \frac{2}{ar} - \frac{1}{a^2} \right)\Psi -\frac{ke^2}{r}\cdot\Psi
\end{align*}
If we require this equation to hold for all $r$, then we must have equality for both the terms of the form
$(\text{constant})\times\Psi$ and for those of the form $(\text{constant}/r)\times\Psi$. That means
\begin{align*}
  E &= -\frac{\hbar^2}{2ma^2} \\
\intertext{and}
  0 &=  \frac{\hbar^2}{mar} -\frac{ke^2}{r}\eqquad.
\end{align*}
These two equations can be solved for the unknowns $a$ and $E$, giving
\begin{align*}
  a &= \frac{\hbar^2}{mke^2} \\
\intertext{and}
  E &= -\frac{mk^2e^4}{2\hbar^2}\eqquad,
\end{align*}
where the result for the energy agrees with the equation given on p.~\pageref{end-approx-hydrogen-energies}.
The calculation of the normalization constant $u$ is relegated to homework problem \ref{hw:h-atom-normalization}.
\end{eg}

\begin{eg}{Wave phases in the hydrogen molecule}\label{eg:h2-details}
In example \ref{h2-bond} on page \pageref{h2-bond}, I argued that the existence of the $\zu{H}_2$ molecule
could essentially be explained by a particle-in-a-box argument: the molecule is a bigger box than an
individual atom, so each electron's wavelength can be longer, its kinetic energy lower.
Now that we're in possession of a mathematical expression for the wavefunction of the hydrogen
atom in its ground state, we can make this argument a little more rigorous and detailed.
Suppose that two hydrogen atoms are in a relatively cool sample of monoatomic hydrogen gas.
Because the gas is cool, we can assume that the atoms are in their ground states. Now suppose
that the two atoms approach one another. Making use again of the assumption that the gas is cool, it
is reasonable to imagine that the atoms approach one another slowly. Now the atoms come a little
closer, but still far enough apart that the region between them is classically forbidden. Each
electron can tunnel through this classically forbidden region, but the tunneling probability is
small. Each one is now found with, say, 99\% probability in its original home, but with 1\%
probability in the other nucleus. Each electron is now in a state consisting of a superposition
of the ground state of its own atom with the ground state of the other atom. There are two peaks
in the superposed wavefunction, but one is a much bigger peak than the other.

An interesting question now arises. What are the relative phases of the two electrons?
As discussed on page \pageref{fig:electron-wave-phase}, the \emph{absolute} phase of an electron's
wavefunction is not really a meaningful concept. Suppose atom A contains electron Alice, and
B electron Bob. Just before the collision, Alice may have wondered, ``Is my phase
positive right now, or is it negative? But of course I shouldn't ask myself such silly questions,''
she adds sheepishly.

<%
  fig(
    'h2-phase',
    %q{%
      Example \ref{eg:h2-details}.
    },
    {
      'width'=>'wide',
      'sidecaption'=>true
    }
  )
%>

But \emph{relative} phases \emph{are} well defined. As the two atoms draw closer and closer together,
the tunneling probability rises, and eventually gets so high that each electron is spending essentially
50\% of its time in each atom. It's now reasonable to imagine that either one of two possibilities could
obtain. Alice's wavefunction could either look like \subfigref{h2-phase}{1}, with the two peaks in
phase with one another, or it could look like \subfigref{h2-phase}{2}, with opposite phases. Because
\emph{relative} phases of wavefunctions are well defined, states 1 and 2 are physically 
distinguishable.\footnote{The reader who has studied chemistry may find it helpful to make contact
with the terminology and notation used by chemists. The state represented by
pictures 1 and 4 is known as a
$\sigma$ orbital, which is a type of ``bonding orbital.'' The state in 2 and 3 is a $\sigma^*$, a kind of
``antibonding orbital.'' Note that although we will not discuss electron spin or the Pauli exclusion
principle until sec.~\ref{sec:intrinsic-spin}, p.~\pageref{sec:intrinsic-spin}, those considerations
have no effect on this example, since the two electrons can have opposite spins.}
In particular, the kinetic energy of state 2 is much higher (by about 5 eV); roughly speaking, it is like the two-hump
wave pattern of the particle in a box, as opposed to 1, which looks roughly like the one-hump pattern
with a much longer wavelength. Not only that, but an electron in state 1 has a large probability of being found in the
central region, where it has a large negative electrical energy due to its interaction with both protons.
State 2, on the other hand, has a low probability of existing in that region. (This effect also equals
about 5 eV.) Thus state 1 represents the
true ground-state wavefunction of the $\zu{H}_2$ molecule, and putting both Alice and Bob in that state
results in a lower energy than their total energy when separated, so the molecule is bound, and will not
fly apart spontaneously.

State \subfigref{h2-phase}{3}, on the other hand, is not physically distinguishable from \subfigref{h2-phase}{2},
nor is \subfigref{h2-phase}{4} from \subfigref{h2-phase}{1}. Alice may say to Bob, ``Isn't it wonderful that
we're in state 1 or 4? I love being stable like this.'' But she knows it's not meaningful to ask herself
at a given moment which state she's in, 1 or 4.
\end{eg}

<% end_sec('sch-three-d') %>
<% begin_sec("Use of complex numbers",nil,'complex-wavefunction') %>

\index{wavefunction!complex numbers in}\index{complex numbers!in quantum physics}
In a classically forbidden region, a particle's total
energy, $U+K$, is less than its $U$, so
its $K$ must be negative. If we want to keep believing
in the equation $K=p^2/2m$, then apparently the
momentum of the particle is the square root of a negative
number. This is a symptom of the fact that the Schr\"odinger
equation fails to describe all of nature unless the
wavefunction and various other quantities are allowed to be
complex numbers. In particular it is not possible to
describe traveling waves correctly without using complex wavefunctions.
Complex numbers were reviewed in subsection \ref{subsec:complex-numbers}, p.~\pageref{subsec:complex-numbers}.

<%
  fig(
    'complex-wavefunction',
    %q{%
      1. Oscillations can go back and forth, but it's also possible for them to move along a path that bites its
      own tail, like a circle. Photons act like one, electrons like the other.\\\\
      2. Back-and-forth oscillations can naturally be described by a segment taken from the real number line, and
      we visualize the corresponding type of wave as a sine wave. Oscillations around a closed path relate more
      naturally to the complex number system. The complex number system has rotation built into its
      structure, e.g., the sequence 1, $i$, $i^2$, $i^3$, \ldots rotates around the unit circle in 90-degree increments.\\\\
      3. The double slit experiment embodies the one and only mystery of quantum physics. Either type of wave
      can undergo double-slit interference.
    },
    {
      'width'=>'wide',
      'sidecaption'=>true
    }
  )
%>

This may seem like nonsense, since real numbers are the only
ones that are, well, real! Quantum mechanics can always be
related to the real world, however, because its structure is
such that the results of measurements always come out to be
real numbers. For example, we may describe an electron as
having non-real momentum in classically forbidden regions,
but its average momentum will always come out to be real
(the imaginary parts average out to zero), and it can never
transfer a non-real quantity of momentum to another particle.

A complete investigation of these issues is beyond the scope
of this book, and this is why we have normally limited
ourselves to standing waves, which can be described with
real-valued wavefunctions. Figure \figref{complex-wavefunction}
gives a visual depiction of the difference between real and
complex wavefunctions. The following remarks may also be helpful.

Neither of the graphs in \subfigref{complex-wavefunction}{2} should be
interpreted as a path traveled by something. This isn't anything mystical about
quantum physics. It's just an ordinary fact about waves, which we first encountered
in sec.~\ref{sec:wave-motion}, p.~\pageref{sec:wave-motion},
where we saw the distinction between the motion
of a wave and the motion of a wave pattern. In \emph{both} examples in \subfigref{complex-wavefunction}{2},
the wave pattern is moving in a straight line to the right.

The helical graph in \subfigref{complex-wavefunction}{2}
shows a complex wavefunction whose value rotates around a circle in the
complex plane with a frequency $f$ related to its energy by $E=hf$. As it does
so, its squared magnitude $|\Psi|^2$ stays the same, so the corresponding probability stays
constant. Which direction does it rotate? This direction is purely a matter of convention,
since the distinction between the symbols $i$ and $-i$ is arbitrary --- both are equally valid
as square roots of $-1$. We can, for example, arbitrarily say that electrons with positive energies
have wavefunctions whose phases rotate counterclockwise, and as long as we follow that rule
consistently within a given calculation, everything will work. Note that it is not possible
to define anything like a right-hand rule here, because the complex plane shown in
the right-hand side of \subfigref{complex-wavefunction}{2} doesn't represent two dimensions of
physical space; unlike a screw going into a piece of wood, an electron doesn't have a direction
of rotation that depends on its direction of travel.

\begin{eg}{Superposition of complex wavefunctions}
\egquestion
The right side of figure \subfigref{complex-wavefunction}{3} is a cartoonish representation of
double-slit interference; it depicts the situation at the center, where symmetry guarantees that
the interference is constructive. Suppose that at some off-center point, the two wavefunctions
being superposed are $\Psi_1=b$ and $\Psi_2=bi$, where $b$ is a real number with units.
Compare the probability of finding the electron at this position with what it would have been
if the superposition had been purely constructive, $b+b=2b$.

\eganswer
The probability per unit volume is proportional to the square of the magnitude of the total
wavefunction, so we have
\begin{equation*}
  \frac{P_{\text{off center}}}{P_{\text{center}}} 
         = \frac{|b+bi|^2}{|b+b|^2} = \frac{1^2+1^2}{2^2+0^2} = \frac{1}{2}\eqquad.
\end{equation*}
\end{eg}

Figure \figref{rainbow} shows a method for visualizing complex wavefunctions. The idea
is to use colors to represent complex numbers, according to the arbitrary convention
defined in figure \subfigref{rainbow}{1}. Brightness indicates magnitude, and the rainbow
hue shows the argument. Because this representation can't be understood in a black and white
printed book, the figure is also reproduced on the back cover of printed copies.
To avoid any confusion, note that the use of rainbow colors does
not mean that we are representing actual visible light. In fact, we will be using these
visual conventions to represent the wavefunctions of a material particle such as an electron.
It is arbitrary that we use red for positive real numbers and blue-green for negative numbers,
and that we pick a handedness for the diagram such that going from red toward yellow means
going counterclockwise. Although physically the rainbow is a linear spectrum, we are not
representing physical colors here, and we are exploiting the fact that the human brain
tends to perceive color as a circle rather than a line, with violet and red being perceptually
similar. One of the limitations of this representation is that brightness is limited, so we
can't represent complex numbers with arbitrarily large magnitudes.
%

<%
  fig(
    'rainbow',
    %q{%
      1.~A representation of complex numbers using color and brightness.
      2.~A wave traveling toward the right.
      3.~A wave traveling toward the left.
      4.~A standing wave formed by superposition of waves 2 and 3.
      5.~A two-dimensional standing wave.
      6.~A double-slit diffraction pattern.
    },
    {
      'width'=>'wide',
      'sidecaption'=>true
    }
  )
%>

Figure \subfigref{rainbow}{2} shows a traveling wave as it propagates to the right.
The standard convention in physics is that for a wave moving in a certain direction,
the phase in the forward direction is farther counterclockwise in the complex plane, and
you can verify for yourself that this is the case by comparing with the convention
defined by \subfigref{rainbow}{1}. The function being plotted here is $\Psi=e^{ikx}$,
where $k=2\pi/\lambda$ is the wavenumber. For the use of the complex exponential, see sec.~\ref{subsec:euler-formula},
p~.\pageref{subsec:euler-formula}; it simply represents a point on the unit circle in the
complex plane. The wavelength $\lambda$
is a constant and can be measured, for example, from one yellow point to the next. The wavelength
is \emph{not} different at different points on the figure, because we are using the colors merely
as a visual encoding of the complex numbers --- so, for example, a red point on the figure is
not a point where the wave has a longer wavelength than it does at a blue point.

Figure \subfigref{rainbow}{3} represents a wave traveling to the left.

Figure \subfigref{rainbow}{4} shows a standing wave created by superimposing the traveling
waves from \subfigref{rainbow}{2} and \subfigref{rainbow}{3}, $\Psi_4=(\Psi_2+\Psi_3)/2$. (The reason
for the factor of 2 is simply that otherwise some portions of $\Psi_4$ would have magnitudes too great to be
represented using the available range of brightness.) All points on this wave have real values,
represented by red and blue-green. We made the superposition real by an appropriate choice of
the phases of $\Psi_2$ and $\Psi_3$. This is always possible to do when we have a standing wave,
but it is \emph{only} possible for a standing wave, and this is the reason for all of the disclaimers
in the captions of previous figures in which I took the liberty of representing a traveling wave as
a sine-wave graph.

Figure \subfigref{rainbow}{5} shows a two-dimensional standing wave of a particle in a box,
and \subfigref{rainbow}{6} shows a double-slit interference pattern. (In the latter, I've cheated
by making the amplitude of the wave on the right-hand half of the picture much greater than it
would actually be.)

\begin{eg}{A paradox resolved}
Consider the following paradox. Suppose we have an electron that is traveling wave, and
its wavefunction looks like a wave-train consisting of 5 cycles of a sine wave. Call the distance
between the leading and trailing edges of the wave-train $L$, so that $\lambda=L/5$.
By sketching the wave, you can easily check that
there are 11 points where its value equals zero. Therefore at a particular moment in time, there are 11 points
where a detector has zero probability of detecting the electron.

But now consider how this would look in a frame of reference where the electron is moving
more slowly, at one fifth of the speed we saw in the original frame. In this frame, $L$ is
the same, but $\lambda$ is five times greater, because $\lambda=h/p$. Therefore in this frame
we see only one cycle in the wave-train. Now there are only 3 points where the probability
of detection is zero. But how can this be? All observers, regardless of their frames of reference, should agree
on whether a particular detector detects the electron.

The resolution to this paradox is that it starts from the assumption that we can depict
a traveling wave as a real-valued sine wave, which is zero in certain places. Actually, we can't.
It has to be a complex number with a rotating phase angle in the complex plane, as in figure
\subfigref{rainbow}{2}, and a
\emph{constant} magnitude.
\end{eg}

\startdq

\begin{dq}
The zero level of interaction energy $U$ is arbitrary, e.g., it's equally valid to pick the zero of
gravitational energy to be on the floor of your lab or at the ceiling. Suppose we're doing the double-slit
experiment, \subfigref{complex-wavefunction}{3}, with electrons. We define the zero-level of $U$ so that
the total energy $E=U+K$ of each electron is positive. and we observe a certain interference
pattern like the one in figure \figref{ccd-diffraction} on p.~\pageref{fig:ccd-diffraction}. What happens
if we then redefine the zero-level of $U$ so that the electrons have $E<0$?
\end{dq}

\pagebreak

\begin{dq}\label{dq:qm-cons-of-prob}
The top panel of the
figure shows a series of snapshots in the motion of two pulses on a coil spring, one negative and one positive,
as they move toward one another and superpose. The final image is very close to the moment at which
the two pulses cancel completely. 
The following discussion is simpler if we consider infinite sine waves rather than pulses.
How can the cancellation of two such mechanical waves be reconciled with conservation of energy?
What about the case of colliding electromagnetic waves? 

Quantum-mechanically, the issue isn't conservation of energy, it's conservation of probability,
i.e., if there's initially a 100\%
probability that a particle exists somewhere, we don't want the probability to be more than or
less than 100\%
at some later time. What happens when the colliding waves have
real-valued wavefunctions $\Psi$? Now consider the sketches of complex-valued wave pulses
shown in the bottom panel of the figure as they are getting ready to collide.
\end{dq}

<% marg(300) %>
<%
  fig(
    'superposition-cancellation',
    ''
  )
%>
<% end_marg %>

<% end_sec('complex-wavefunction') %>

<% begin_sec("Linearity of the Schr\\\"odinger equation",nil,'linearity-of-schrodinger') %>

Some mathematical relationships and operations are \emph{linear}, and some are not.
For example, $2\times(3+2)$ is the same as $2\times3+2\times2$, but $\sqrt{1+1}\ne\sqrt{1}+\sqrt{1}$.
Differentiation is a linear operation, $(f+g)'=f'+g'$. The Schr\"odinger equation is
built out of derivatives, so it is linear as well. That is, if $\Psi_1$ and $\Psi_2$ are
both solutions of the Schr\"odinger equation, then so is $\Psi_1+\Psi_2$. Linearity
normally implies linearity with respect both to addition and to multiplication by a scalar.
For example, if $\Psi$ is a solution, then so is $\Psi+\Psi+\Psi$, which is the same as $3\Psi$.

Linearity guarantees that the phase of a wavefunction makes no difference as to its validity
as a solution to the Schr\"odinger equation. If $\sin kx$ is a solution, then so is the sine wave
$-\sin kx$ with the opposite phase. This fact is logically interdependent with the fact that,
as discussed on p.~\pageref{phase-unobservable-basic}, the phase
of a wavefunction is unobservable.\index{phase in quantum mechanics!not observable}
For measuring devices and humans are material objects
that can be described by wavefunctions. So suppose, for example, that we flip the phase of
all the particles inside the entire laboratory. By linearity,
the evolution of this measurement process is still a valid  solution of the Schr\"odinger equation.

The Schr\"odinger equation is a wave equation, and its
linearity implies that the waves obey the principle of superposition.
In most cases in nature, we find that the principle of superposition for waves is at best an
approximation. For example, if the amplitude of
a tsunami is so huge that the trough of the wave reaches all the way down to the ocean floor,
exposing the rocks and sand as it passes overhead, then clearly there is no way to double the
amplitude of the wave and still get something that obeys the laws of physics. Even at less extreme
amplitudes, superposition is only an approximation for water waves, and so for example it is only
approximately true that when two sets of ripples intersect on the surface of a pond, they pass
through without ``seeing'' each other.

It is therefore natural to ask whether the apparent linearity of the Schr\"odinger equation is only
an approximation to some more precise, nonlinear theory. This is not currently believed to be the
case. If we are to make sense of Schr\"odinger's cat
(sec.~\ref{sec:nonlocality-and-entanglement}, p.~\pageref{schrodingers-cat}),
then the experimenter who sees a live cat and the one
who sees a dead cat must remain oblivious to their other selves, like the ripples on the pond
that intersect without ``seeing'' each other. Attempts to create slightly nonlinear versions
of standard quantum mechanics have been shown to have implausible physical properties, such as
allowing the propagation of signals faster than $c$. (This is known as Gisin's theorem.\label{gisin-theorem}
The original paper, ``Weinberg's non-linear quantum mechanics and supraluminal communications,''
is surprisingly readable and nonmathematical.)\index{Gisin's theorem}

The linearity of the Schr\"odinger equation is what allows us to talk
about its solutions as vectors in a vector space
(p.~\pageref{lin-alg-vector-space}). For example, if $\Psi_1$
represents an unstable nucleus that has not yet gamma decayed, and
$\Psi_2$ is its state after the decay, then any superposition
$\alpha\Psi_1+\beta\Psi_2$, with real or complex coefficients $\alpha$
and $\beta$, is a possible wavefunction, and we can notate this as a
vector, $\begin{pmatrix}\alpha\\\beta\end{pmatrix}$, in a
two-dimensional vector space.

People wrestling with the weirdness of Schr\"odinger's cat sometimes say that
it's impossible to have a superposition of the live cat and the dead cat.
Not true. The linearity of the Schr\"odinger equation guarantees that we
can take any wavefunctions $\Psi_1$ and $\Psi_2$ and superpose them, and this
is exactly what happens, according to quantum physics, when we do the experiment.
What \emph{is} impossible, for all practical purposes, is to observe any type of
interaction between the live and dead cats, such as wave interference,
which we estimated on p.~\pageref{cat-fringes} and found to be undetectable, due to the
extremely small wavelength.

<% end_sec('linearity-of-schrodinger') %>

<% begin_sec("The inner product and observables",nil,'quantum-inner-product-and-observables') %>
<% begin_sec("The inner product",nil,'quantum-inner-product') %>\index{inner product!quantum mechanics}
In sec.~\ref{sec:linear-algebra}, p.~\pageref{inner-product}, we discussed the idea of an inner product, which is
an operation on a vector space that acts like the dot product, i.e., it takes two vectors as inputs and gives
back a scalar as an output. We also saw that any time we had a way of measuring magnitudes, we automatically
got an inner product for free. In quantum mechanics, we do have a way of measuring magnitudes, which is the total
probability of a given wavefunction. We even have a convenient notation for this, the bra-ket notation\label{bra-ket notation}
introduced in sec.~\ref{subsec:bra-ket}, p.~\pageref{subsec:bra-ket}. In example \ref{eg:particle-in-box-norm-and-phase},
we had a wavefunction $| <% eqn_image("eqn-wave2") %> \rangle$, and the fact of its being normalized was written as
$\langle <% eqn_image("eqn-wave2") %> | <% eqn_image("eqn-wave2") %> \rangle = 1$. This is a statement that the
total probability for \emph{something} to happen must be 1, but we can also think of it as a statement
that the ``magnitude'' of <% eqn_image("eqn-wave2") %> has a certain value. Because we have a way of defining magnitudes
of wavefunctions, we automatically get an inner product.

If we're dealing with wavefunctions that are expressed as functions of position, then it's pretty clear
how to define an appropriate inner product: $\langle u | v \rangle = \int u^*v \der x$. We need to use the
complex conjugate $u^*$ rather than just $u$, for the following reason. When we take the inner product
of a wavefunction with itself, it has a probability interpretation. Probabilities are always real and positive.
So we need to set things up in such a way that $\langle u | u \rangle$ is always real and positive.
When we take $\langle u | u \rangle$,
the thing inside the integral is $u^*u$, which is the same as $|u|^2$, and this is indeed real and positive,
so when we integrate it we get an answer that is real and positive.

Note the similarity between the expression $\int u^*v \der x$ and the expression
$u_xv_x+u_yv_y+u_zv_z$ for a dot product: the integral is a continuous sum, and the dot product is
a discrete sum.

We recall from our previous discussion (p.~\pageref{inner-product}) that
an inner product can generally be described as a measure of how similar two vectors are.
For example, in the Euclidean plane, vectors that are perpendicular to each other have
a dot product of zero, which tells you that they lie along lines that are completely different.
When two vectors have an inner product of zero, we say that they are \emph{orthogonal}.

<% marg(300) %>
<%
fig('qm-inner-product-properties',%q{
  Properties of the inner product. Except for the complex conjugates, these are the same as the properties of
  the dot product from Euclidean geometry.
},{'anonymous'=>true,'text'=>
%q{
    \begin{align*}
      & \langle u | v \rangle = \langle v | u \rangle^* \\\\
      & \langle u | \alpha v+\beta w \rangle = \alpha\langle u | v \rangle + \beta\langle u | w \rangle \\\\
      & \langle \alpha u+\beta v | w \rangle = \alpha^*\langle u | w \rangle + \beta^*\langle v | w \rangle .
    \end{align*}
  }
}
)
%>
<% end_marg %>

Physically, two wavefunctions have a zero inner product if and only if they are completely distinguishable from each other
by the measurement of some observable. By analogy with vectors in Euclidean space, we say that
the two wavefunctions are orthogonal.\label{orthogonal-wavefunctions}
For example, $\langle <% eqn_image("eqn-wave1") %> | <% eqn_image("eqn-wave2") %> \rangle=0$, as can be verified
from the integral $\int_0^\pi \sin x \sin 2x \der x=0$. These states are also distinguishable by measuring
either their momentum or their energy.

<%
  fig(
    'inner-product-interpretation-simple',
    %q{%
      Some examples of interpretation of the inner product.
    },
    {
      'width'=>'wide',
      'sidecaption'=>true
    }
  )
%>

% The 0.81 is 8/pi^2, because a triangle wave is \Sum (1/n^2)\cos(nx), for n odd.

Suppose that $u$ and $v$ are both properly normalized wavefunctions. If $|\langle u|v \rangle|=1$, then the
states are identical.\footnote{If the inner product is, for example, $-1$, then the wavefunctions differ only by
an unobservable difference in phase, so they really describe the same state.}
If $\langle u|v \rangle=0$, then $u$ and $v$ are completely distinguishable from one another. There is
also the intermediate case where $\langle u|v \rangle$ has a magnitude greater than 0 but less than 1.
In this case, we could say that $u$ is a mixture of $v$ plus some other state $w$ that is distinguishable
from $v$, i.e., that
\begin{equation*}
  |u \rangle = \alpha |v \rangle + \beta |w \rangle.
\end{equation*}
where $\langle v|w \rangle=0$. We then have
\begin{equation*}
  \langle u|v \rangle = (\alpha \langle v | + \beta \langle w |)|v \rangle = \alpha.
\end{equation*}
Now suppose that we make measurements capable of determining whether or not the system
is in the state $v$. If the system is prepared in state $u$, and we make these measurements on it, then
by the linearity of the Schr\"odinger equation, the result is that the measuring apparatus or observer ends up in
a Schr\"odinger's-cat state that looks like
\begin{equation*}
  \alpha |\text{observed $v$} \rangle + \beta |\text{observed $w$} \rangle.
\end{equation*}
We interpret squares of amplitudes as probabilities, so
\begin{equation*}
  P=|\alpha|^2=|\langle u|v \rangle|^2
\end{equation*}
gives us the probability that we will have observed the state to be $v$. This final leap in the logic,
to a probability interpretation, has felt mysterious to several generations of physicists, but recent
work has clarified the situation somewhat.

<% end_sec('quantum-inner-product') %>
<% begin_sec("Observables",nil,'quantum-observables') %>
When I was in college, my stepmother attempted to make me more hip, more artistic, and less uncool.
I would go over for a visit sometimes on the weekend, and she would sit me down, pour us each a glass
of wine, and draw me out. In one of these conversations, I insisted that love didn't exist, because
there was no way to measure it. The physicist's way of thinking, which by that time I had already
started to adopt, is that we should distinguish carefully between things that are \emph{observables}
and things that aren't. One of the basic principles of quantum mechanics is that $\langle u|v\rangle=0$
if and only if there is some observable that perfectly distinguishes state $u$ from state $v$.
(We may actually need more than one observable.)

This makes it important to be clear on what really is an observable and what isn't. Informally,
what we mean by an observable is that:
\begin{enumerate}
  \item we can tell by looking at the state what value it has, or at least assign probabilities to values;
  \item it has a single value; and
  \item (optionally) it has some counterpart in classical physics.
\end{enumerate}
The following are all observables:
\begin{itemize}
  \item position
  \item momentum
  \item energy
  \item angular momentum (ch.~\ref{ch:quantum-ang-mom}).
\end{itemize}
The following are not:
\begin{itemize}
  \item phase and normalization (sec.~\ref{subsec:same-state})
  \item time (because many systems, such as an atom, are too simple to act as clocks).
\end{itemize}
A couple of interesting borderline cases are wavelength and angle. Classically, an electron
doesn't have a wavelength, so we would usually choose to talk about its momentum as an observable,
not its wavelength (although they relate via $p=h/\lambda$, so it really doesn't make much
difference). For an electron in an atom, thinking in a simplified picture in two dimensions, we could talk
about its angle in the plane, but this causes problems because angles are not really single-valued, e.g.,
0 is the same as $360\degunit$.

The fact that position is an observable but time is not is one of the things that makes it difficult
to reconcile quantum mechanics with relativity, which considers time to be just another dimension.
This difficulty has been reconciled for special relativity, but not for general relativity.

<% end_sec('quantum-observables') %>
<% end_sec('quantum-inner-product-and-observables') %>

<% begin_sec("Time evolution and unitarity",nil,'unitarity') %>
<% begin_sec("The simplest cases of time evolution",nil,'simple-time-evolution') %>
So far we have not said too much about how a wavefunction changes with time, except that a state of definite energy $E$ has some
frequency given by $E=\hbar\omega$. As a simple example, consider a particle in a box. Let's say that it's initially in the ground state,
which is a standing wave like <% eqn_image("eqn-wave1","../share/quantum/figs") %>. It has some energy, so we can determine its
frequency and period. Based on experience with standing waves on a string, we expect that after
half a period it will look like <% eqn_image("eqn-wave1-flipped","../share/quantum/figs") %>. The problem is how it gets from
the frown to the smile. If it behaved like a wave on a string, it would go from <% eqn_image("eqn-wave1","../share/quantum/figs") %>
to <% eqn_image("eqn-wave1-flat","../share/quantum/figs") %> to <% eqn_image("eqn-wave1-flipped","../share/quantum/figs") %>.
But passing through the flat intermediate state won't work, because of the fundamental structure of quantum mechanics,
which includes the principle of state fundamentalism (p.~\pageref{state-fundamentalism}). The state, represented by our
picture of the wavefunction, is supposed to be all there is to know about the system. But if someone presented us with
the zero wavefunction <% eqn_image("eqn-wave1-flat","../share/quantum/figs") %>, we would have \emph{no} information at all
about the state.

So the wavefunction's amplitude can't oscillate  back and forth along the real number line between positive and negative values,
as it would for a wave on a string. What it actually does is to spin around in a circle in the complex plane. That is, instead
of going like
\begin{equation*}
1\times<% eqn_image("eqn-wave1","../share/quantum/figs") %> \qquad
0\times<% eqn_image("eqn-wave1","../share/quantum/figs") %> \qquad
-1\times<% eqn_image("eqn-wave1","../share/quantum/figs") %>, \qquad \text{[wrong]}
\end{equation*}
it goes like
\begin{equation*}
1\times<% eqn_image("eqn-wave1","../share/quantum/figs") %> \qquad
-i\times<% eqn_image("eqn-wave1","../share/quantum/figs") %> \qquad
-1\times<% eqn_image("eqn-wave1","../share/quantum/figs") %>. \qquad \text{[right]}
\end{equation*}
(We have $-i$ here rather than $i$ because the convention is to have it spin clockwise.)
This is enough to define the time-dependence of any state that has a definite energy: the energy tells us
a frequency $\omega$, and then we know that the time-evolution of the state is simply that it spins its
phase like $e^{-i\omega t}$, as in Euler's formula (sec.~\ref{subsec:euler-formula}, p.~\pageref{subsec:euler-formula}).

<% marg() %>
<%
  fig(
    'rainbow-traveling',
    %q{%
      A quantum-mechanical traveling wave with a definite energy, and therefore a definite frequency.
      Complex values are represented as colors, according to the conventions introduced in figure \figref{rainbow},
      p.~\pageref{fig:rainbow}. The wave should actually be thought of as extending infinitely far in both directions,
      but I've drawn a finite piece of it in order to make it more visually obvious that it's traveling to the right.
    }
  )
%>
<% end_marg %>


A particularly nice example is a traveling wave with a definite
energy, figure \figref{rainbow-traveling}. Frozen at one moment in
time, such a wave looks like $e^{ikx}$, which we visualize as
a repeating rainbow. If we let this wave travel, then as we saw in ch.~\ref{ch:waves},
its rigidly gliding motion should be described by a function whose input is of the form
$kx-\omega t$. In other words, we have $\Psi=e^{i(kx-\omega t)}$. An observer who is
watching the wave go by will say that a certain point with a fixed phase, say $\Psi=1$,
which is red in the figure,
is moving to the right at a certain speed. But an observer who just stays in one plays
as the wave washes over her will say that $\Psi$ is spinning around in a circle in the
complex plane. To see this more explicitly, we can break up the exponential into separate
factors, $\Psi=e^{ikx}e^{-i\omega t}$. The time-dependent factor of $e^{-i\omega t}$ is
just what we described a moment ago: a phase spinning clockwise in the complex plane.

For states that are not states of definite energy, the time-evolution is a little more
complicated and is given by an equation known as the time-dependent Schr\"odinger equation,
which we will not write down explicitly here --- but for states of definite energy all the equation describes
is the kind of phase-spinning described above.
<% end_sec('simple-time-evolution') %>

<% begin_sec("Unitarity",nil,'unitarity-subsec') %>

In discussion question \ref{dq:qm-cons-of-prob} on p.~\pageref{dq:qm-cons-of-prob}, we considered
two traveling waves that collided head-on and superposed, and we convinced ourselves that
probability would be conserved. 
It's possible to prove that the time-evolution of the wavefunction always
results in conservation of probability.

To put this in real-world terms, suppose that your
history teacher calls on you in class and asks you what happened on December 7, 1941. With
a straight face, you answer, ``Professor, I can guarantee that \emph{something} happened on that day.''
``That's correct. And what about August 6, 1945?'' ``Yes, something also happened on that day.''
In other words, if we have a properly normalized wavefunction at a certain time, then we expect it
to remain properly normalized at all later times.

The time evolution is also completely deterministic, so that if we know
$\Psi$ initially, we can always predict it in the future. We can also ``predict'' backward in time,
so that the system's history can always be recovered
from knowledge of its present state. Thus there is never any loss of information over time.

Summarizing, we have the following important principle:

\begin{important}[Unitary
         evolution of the wavefunction]\index{unitary evolution of the wavefunction}\label{unitarity-postulate}
The wavefunction evolves over time in a deterministic
and \emph{unitary} manner,
meaning that probability is conserved and information is never lost.
\end{important}

\noindent The word ``unitary'' comes from linear algebra, which gives us the following more crisp mathematical
way of formulating this principle.
A unitary transformation\index{unitary operator}
is one that preserves inner products. That is, $\mathcal{O}$ is unitary if
$\langle\mathcal{O}u|\mathcal{O}v\rangle=\langle u|v\rangle$. This is similar to the way in which
rotations preserve dot products in Euclidean geometry. The linearity of the Schr\"odinger equation
guarantees that the evolution of the wavefunction from one time to another time can be represented
by a linear operator $\mathcal{O}$. We require this to be unitary, and it is unitary for the
Schr\"odinger equation.

Since we think of quantum mechanics as being all about randomness, this determinism may seem surprising.
But determinism in the time-evolution of the wavefunction isn't the same as determinism in the results
of experiments as perceived and recorded by a human brain. Suppose that you prepare a uranium atom in
its ground state, then wait one half-life and observe whether or not it has decayed, as in the thought
experiment of Schr\"odinger's cat (p.~\pageref{schrodingers-cat}). There is no uncertainty or randomness about the wavefunction of the
whole system (atom plus you) at the end. We know for sure what it looks like.
It consists of an equal superposition of two states,
one in which the atom has decayed and your brain has observed that fact, and one in which the atom has
not yet decayed and that fact is instead recorded in your brain. 

As a possible example of a violation of unitarity, in an exotic context, consider
the disappearance of matter into a black hole.
If I throw my secret teenage diary into a black hole, then it contributes a little bit to the black
hole's mass, but the embarrassing information on the pages is lost forever. This loss of information
seems to imply nonunitarity. This is one of several arguments suggesting that quantum mechanics cannot
fully handle the gravitational force. Thus although physicists currently seem to possess a completely successful
theory of gravity (Einstein's theory of general relativity) and a completely successful theory of
the microscopic world (quantum mechanics), the two theories are irreconcilable, and
we can only make educated guesses, for example, about the behavior of a hypothetical microscopic black hole.

<% end_sec('unitarity-subsec') %>

<% end_sec('unitarity') %>




<% begin_hw_sec(vfill:true) %>

\newcommand{\eqnimage}[1]{\raisebox{-.2\height}{\includegraphics{../share/quantum/figs/#1}}}
% duplicated in FAC, ac/b.rbtex
% used to have \includegraphics[resolution=300], but that now causes an error:
%       https://tex.stackexchange.com/questions/301494/how-to-use-includegraphics




<% begin_hw('hydrogen-scale') %>__incl(hw/hydrogen-scale)<% end_hw() %>

<% begin_hw('electroninproton') %>__incl(hw/electroninproton)<% end_hw() %>

<% begin_hw('h-atom-normalization') %>__incl(hw/h-atom-normalization)<% end_hw() %>

\pagebreak

<% begin_hw('quantumho') %>__incl(hw/quantumho)<% end_hw() %>

<% begin_hw('three-d-forbidden') %>__incl(hw/three-d-forbidden)<% end_hw() %>

<% begin_hw('three-d-box') %>__incl(hw/three-d-box)<% end_hw() %>

\pagebreak

<% begin_hw('tritium-decay') %>__incl(hw/tritium-decay)<% end_hw() %>

<% begin_hw('square-wavefunction') %>__incl(hw/square-wavefunction)<% end_hw() %>

<% begin_hw('ho-orthogonal') %>__incl(hw/ho-orthogonal)<% end_hw() %>

\pagebreak

<% begin_hw('particle-in-a-box-superpos') %>__incl(hw/particle-in-a-box-superpos)<% end_hw() %>

<% end_hw_sec %>


<% end_chapter() %>
