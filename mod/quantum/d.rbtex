<%
  require "../eruby_util.rb"
%>

<%
  chapter(
    '16',
    %q{Matter as a wave},
    'ch:matter-as-a-wave',
    %q{Dorothy melts the Wicked Witch of the West.},
    {'opener'=>'melting-witch','sidecaption'=>true,'anonymous'=>true}
  )
%>


\epigraphlong{[In] a few minutes I shall be all melted... I have been
wicked in my day, but I never thought a little girl like you
would ever be able to melt me and end my wicked deeds.
Look out --- here I go!}{The \index{Wicked Witch of the West}Wicked Witch
 of the West}

As the Wicked Witch learned the hard way, losing molecular
cohesion can be unpleasant. That's why we should be very
grateful that the concepts of quantum physics apply to
matter as well as light. If matter obeyed the laws of
classical physics, \index{molecules!nonexistence in
classical physics}molecules wouldn't exist.

Consider, for example, the simplest atom, hydrogen. Why does
one hydrogen atom form a chemical bond with another hydrogen
atom? Roughly speaking, we'd expect a neighboring pair of
hydrogen atoms, A and B, to exert no force on each other
at all, attractive or repulsive: there are two repulsive
interactions (proton A with proton B and electron A with
electron B) and two attractive interactions (proton A with
electron B and electron A with proton B). Thinking a
little more precisely, we should even expect that once the
two atoms got close enough, the interaction would be
repulsive. For instance, if you squeezed them so close
together that the two protons were almost on top of each
other, there would be a tremendously strong repulsion
between them due to the $1/r^2$ nature of the electrical
force. The repulsion between the electrons would not be as
strong, because each electron ranges over a large area, and
is not likely to be found right on top of the other
electron. Thus hydrogen molecules should not exist according
to classical physics.

Quantum physics to the rescue! As we'll see shortly, the
whole problem is solved by applying the same quantum
concepts to electrons that we have already used for photons.

<% begin_sec("Electrons as waves") %>
<% begin_sec("Wavelength related to momentum",nil,'de-broglie') %>

\index{electron!as a wave}
We started our journey into quantum physics by studying the
random behavior of \emph{matter} in radioactive decay, and
then asked how randomness could be linked to the basic laws
of nature governing \emph{light}. The probability interpretation
of wave-particle duality was strange and hard to accept, but
it provided such a link. It is now natural to ask whether
the same explanation could be applied to matter. If the
fundamental building block of light, the photon, is a
particle as well as a wave, is it possible that the basic
units of matter, such as electrons, are waves as well as particles?

A young French aristocrat studying physics, Louis \index{de
Broglie!Louis}de Broglie (pronounced ``broylee''), made
exactly this suggestion in his 1923 Ph.D. thesis. His idea
had seemed so farfetched that there was serious doubt about
whether to grant him the degree. Einstein was asked for his
opinion, and with his strong support, de Broglie got his degree.

Only two years later, American physicists C.J. \index{Davisson!C.J.}Davisson
and L. \index{Germer, L.}Germer confirmed de Broglie's idea
by accident. They had been studying the scattering of
electrons from the surface of a sample of nickel, made of
many small crystals. (One can often see such a crystalline
pattern on a brass doorknob that has been polished by
repeated handling.) An accidental explosion occurred, and
when they put their apparatus back together they observed
something entirely different: the scattered electrons were
now creating an interference pattern! This dramatic proof of
the wave nature of matter came about because the nickel
sample had been melted by the explosion and then resolidified
as a single crystal. The nickel atoms, now nicely arranged
in the regular rows and columns of a crystalline lattice,
were acting as the lines of a diffraction grating. The new
crystal was analogous to the type of ordinary diffraction
grating in which the lines are etched on the surface of a
mirror (a reflection grating) rather than the kind in which
the light passes through the transparent gaps between the
lines (a transmission grating).

<%
  fig(
    'neutron-interference',
    %q{%
      A double-slit interference pattern
       made with neutrons. (A. Zeilinger, R. G\"{a}hler, C.G. Shull,
       W. Treimer, and W. Mampe, \emph{Reviews of Modern Physics}, Vol. 60, 1988.)
    },
    {
      'width'=>'wide'
    }
  )
%>

Although we will concentrate on the wave-particle duality of
electrons because it is important in chemistry and the
physics of atoms, all the other ``particles'' of matter
you've learned about show wave properties as well.
Figure \figref{neutron-interference}, for instance, shows a wave interference
pattern of neutrons.  

It might seem as though all our work was already done for
us, and there would be nothing new to understand about
electrons: they have the same kind of funny wave-particle
duality as photons. That's almost true, but not quite. There
are some important ways in which electrons differ significantly from photons:
\begin{enumerate}
\item Electrons have mass, and photons don't. 
\item Photons always move at the speed of light, but electrons
can move at any speed less than $c$. 
\item Photons don't have electric charge, but electrons do, so
electric forces can act on them. The most important example
is the atom, in which the electrons are held by the electric
force of the nucleus.
\item Electrons cannot be absorbed or emitted as photons are.
Destroying an electron or creating one out of nothing would
violate conservation of charge.
\end{enumerate}
(In section \ref{sec:atom} we will learn of one more fundamental way in
which electrons differ from photons, for a total of five.)

Because electrons are different from photons, it is not
immediately obvious which of the photon equations from
chapter \ref{ch:light-as-a-particle} can be applied to electrons as well. 
With hindsight, we know that there is a simple and consistent way of
putting this all together. As a preliminary in order to make the
notation simpler, we define a version of Planck's constant
$\hbar=h/2\pi$, read as ``h-bar.'' We also recall from sec.~\ref{subsec:wavenumber}, p.~\pageref{subsec:wavenumber},
the definition of the wavenumber $k=2\pi/\lambda$ along with a few of its properties, table \figref{k-review}.
With these definitions, our two fundamental quantum-mechanical relations are
\begin{align*}
                E  &=  \hbar \omega \qquad \text{[the same as $E=hf$]}  \\
                p  &=  \hbar k      \qquad \text{[the same as $p=h/\lambda$]}
\end{align*}
It's only a slight exaggeration to say that these two equations summarize
all of quantum mechanics. They are true for all the ordinary building blocks of light and matter
in everyday life, and also for every weird creature in the particle-physics zoo.
Each of these equations has a \emph{particle} thing on the left and a \emph{wave} thing
on the right.

<% marg(100) %>
<% fig('k-review','Review of the wavenumber $k$',
  {
    'text'=>%q{
      \\begin{itemize}
        \\item The wavenumber $k=2\\pi/\\lambda$ is inversely related to the wavelength $\\lambda$.
        \\item It has units of radians per meter.
        \\item In three dimensions, $k$ is the magnitude of a the wave-vector $\\vc{k}$ (p.~\\pageref{wave-vector}).
        \\item The wavevector $k$ is to the wavelength as the frequency $\\omega$ is to the period.
      \\end{itemize}
    }
  })
%>

<% end_marg %>

The second equation, although true for photons,
takes on a greater importance for electrons. This is
first of all because the momentum of matter is more likely
to be significant than the momentum of light under ordinary
conditions, and also because force is the transfer of
momentum, and electrons are affected by electrical forces.

\begin{eg}{The wavelength of a cat}\label{eg:cat-wavelength}
\egquestion What is the wavelength of a trotting cat?

\eganswer One may doubt whether the equation $p=\hbar k=h/\lambda$should be
applied to a cat, which is not just a single particle
but a rather large collection of them. Throwing caution to
the wind, however, we estimate the cat's mass at 1 kg
and its trotting speed at 1 m/s. Its wavelength
is therefore roughly
\begin{align*}
        \lambda         &=    \frac{h}{p}  \\
          &=    \frac{h}{mv}  \\
  &= \frac{6.63\times10^{-34}\ \zu{J}\unitdot\sunit}{(1\ \kgunit)(1\ \munit/\sunit)} \\
  &\sim 10^{-33}\ \frac{\left(\kgunit\unitdot\munit^2/\sunit^2\right)\unitdot\sunit}{\kgunit\unitdot\munit/\sunit} \\
        &= 10^{-33}\ \munit
\end{align*}
\end{eg}

The wavelength found in this example is so fantastically
small that we can be sure we will never observe any
measurable wave phenomena with cats or any other
human-scale objects. For example, if we sent the cat through a pair of double
slits separated by 10 cm, then the angular spacing of the diffraction pattern would be
$10^{-32}$ radians, which would make the fringes too close together to be distinguished. This is in agreement with the
correspondence principle.\label{cat-fringes}

Although a smaller mass in the equation $\lambda =h/mv$
does result in a longer wavelength, the wavelength is still
quite short even for individual electrons under typical
conditions, as shown in the following example.

\begin{eg}{The typical wavelength of an electron}
\egquestion Electrons in circuits and in atoms are typically
moving through voltage differences on the order of 1 V,
so that a typical energy is $(e)(1\ \zu{V})$, which is on the
order of $10^{-19}\ \junit$. What is the wavelength of an electron
with this amount of kinetic energy?

\eganswer This energy is nonrelativistic, since it is much
less than $mc^2$. Momentum and energy are therefore related
by the nonrelativistic equation $K=p^2/2m$. Solving
for $p$ and substituting in to the equation for the wavelength, we find
\begin{align*}
                \lambda          &=  \frac{h}{\sqrt{2mK}}    \\
                         &=    1.6\times10^{-9}\ \zu{m}\eqquad.
\end{align*}
This is on the same order of magnitude as the size of an
atom, which is no accident: as we will discuss in the next
chapter in more detail, an electron in an atom can be
interpreted as a standing wave. The smallness of the
wavelength of a typical electron also helps to explain why
the wave nature of electrons wasn't discovered until a
hundred years after the wave nature of light. To scale the
usual wave-optics devices such as diffraction gratings down
to the size needed to work with electrons at ordinary
energies, we need to make them so small that their parts are
comparable in size to individual atoms. This is essentially
what Davisson and Germer did with their nickel crystal.
\end{eg}

<% self_check('longwavelengthelectron',<<-'SELF_CHECK'

These remarks about the inconvenient smallness of electron
wavelengths apply only under the assumption that the
electrons have typical energies. What kind of energy would
an electron have to have in order to have a longer
wavelength that might be more convenient to work with?
  SELF_CHECK
  ) %>

<% end_sec('de-broglie') %>

<% begin_sec("What kind of wave is it?",nil,'what-kind-of-wave') %>

\index{wavefunction!of the electron}\index{electron!wavefunction}
If a sound wave is a vibration of matter, and a photon is a
vibration of electric and magnetic fields, what kind of a
wave is an electron made of? The disconcerting answer is
that there is no experimental ``observable,'' i.e., directly
measurable quantity, to correspond to the electron wave
itself. In other words, there are devices like microphones
that detect the oscillations of air pressure in a sound
wave, and devices such as radio receivers that measure the
oscillation of the electric and magnetic fields in a light
wave, but nobody has ever found any way to measure the
electron wave directly.

<% marg(30) %>
<%
  fig(
    'electron-wave-phase',
    %q{%
      These two electron
       waves are not distinguishable by any measuring device.
    }
  )
%>
<% end_marg %>

We can of course detect the energy (or momentum) possessed by an
electron just as we could detect the energy of a photon using a
digital camera. (In fact I'd imagine that an unmodified digital camera
chip placed in a vacuum chamber would detect electrons just as handily
as photons.) But this only allows us to determine where the wave
carries high probability and where it carries low probability.
Probability is proportional to the square of the wave's amplitude, but
measuring its square is not the same as measuring the wave itself. In
particular, we get the same result by squaring either a positive
number or its negative, so there is no way to determine the positive
or negative sign of an electron wave.  In general, the phase of the
wavefunction is not an observable
thing.\label{phase-unobservable-basic}\index{phase in quantum mechanics!not observable}

This meaninglessness of phase only applies to \emph{absolute} phases.
\emph{Relative} phases do have real-world consequences. For example, figure
\figref{neutron-interference} on p.~\pageref{fig:neutron-interference} shows
a double-slit interference pattern made by neutrons. There are places where we
observe a lot of neutrons, and places where we don't. The places where we don't
see many neutrons are the ones where waves are (at least partially) canceling,
which means that they're out of phase --- \emph{relative} to each other.
As a political metaphor, my wife and I would sometimes, before 2016, cancel
each other out at the polls by voting for different political parties. It was
a matter of opinion who was right, but it was an objective fact that we canceled.

Most physicists tend toward the school of philosophy known
as operationalism, which says that a concept is only
meaningful if we can define some set of operations for
observing, measuring, or testing it. According to a strict
operationalist, then, the electron wave itself is a
meaningless concept, because we can't measure its absolute phase. Nevertheless, it turns out to be one of
those concepts like love or humor that is impossible to
measure and yet very useful to have around. We therefore
give it a symbol, $\Psi $ (the capital Greek letter psi),
and a special name, the electron \emph{wavefunction}
(because it is a function of the coordinates $x$, $y$, and $z$
that specify where you are in space). It would be impossible,
for example, to calculate the shape of the electron wave in
a hydrogen atom without having some symbol for the wave. But
when the calculation produces a result that can be compared
directly to experiment, the final algebraic result will turn
out to involve only $\Psi^2$, which is what is observable, not $\Psi $ itself.

Since $\Psi $, unlike $E$ and $B$, is not directly
measurable, we are free to make the probability equations
have a simple form: instead of having the probability
density equal to some funny constant multiplied by $\Psi^2$,
we simply define $\Psi $ so that the constant of
proportionality is one:
\begin{equation*}
  (\text{probability distribution})  =  |\Psi| ^2\eqquad.  
\end{equation*}
Since the probability distribution has units of $\zu{m}^{-3}$, the units
of $\Psi $ must be $\zu{m}^{-3/2}$. The square of a negative
number is still positive, so the absolute value signs may seem unnecessary,
but as we'll in sec.~\ref{sec:complex-wavefunction}, p.~\pageref{sec:complex-wavefunction},
the wavefunction may in general be a complex number. In fact, only standing waves, not traveling waves,
can really be represented by real numbers, although we will often cheat and draw
pictures of traveling waves as if they were real-valued functions.\label{cheat-with-real-psi}

We have defined a wave as something that can superpose, and a particle as something that you can't
have a fraction of (p.~\pageref{define-particle-and-wave}). Since you can't have a fraction of
an electron, we conclude that if $\Psi$ is a valid wavefunction
for an electron, then $\Psi/2$ should be somehow illegal. We can see this from the requirement
that the probability distribution be normalized (p.~\pageref{normalization-continuous}).

\begin{eg}{Normalization and phase of a particle in a box}\label{eg:particle-in-box-norm-and-phase}
A certain electron confined to a box of length $L$ has a wavefunction
\begin{equation*}
\Psi(x) = \begin{cases}
A \sin (2\pi x/L) & \text{if} \ 0 \le x \le L \\
0             & \text{elsewhere.}
\end{cases}
\end{equation*}
This quantity inside the sine function varies from 0 to $2\pi$, so this is a standing wave with one
wavelength fitting inside the box, <% eqn_image("eqn-wave2") %>. If we pick positive or negative values of $A$ we can have the two
functions in figure \figref{electron-wave-phase}, p.~\pageref{fig:electron-wave-phase}, but this is
a difference in phase, which we don't expect to be physically observable. Normalization requires
that
\begin{align*}
  & \int_0^L |\Psi| ^2 \der x = 1 \\
\intertext{or}
  & |A|^2 \int_0^L  \sin^2(2\pi x/L) \der x = 1.
\end{align*}
This integral can be done using straightforward calculus, but a shortcut is to recognize that the sine
oscillates symmetrically between 0 and 1, and therefore
over any whole number of half-cycles its average value is 1/2. We can therefore find the same result for
the definite integral by replacing the sine with 1/2, and this results in
\begin{equation*}
  |A|^2 = \frac{2}{L}.
\end{equation*}
Two possible values of $A$ are $\sqrt{2/L}$ and $-\sqrt{2/L}$, but these represent physically indistinguishable
states. We could also have a complex value of $A$, as long as $|A|^2$ had the correct value.
\end{eg}

Summarizing the physical moral of example
\ref{eg:particle-in-box-norm-and-phase}, we see that when two
wavefunctions differ by only a constant factor, they actually
represent the same state. The constant factor can indicate nothing
more than a phase difference, which is unobservable, or failure of
normalization, or both. We don't actually want to stop talking completely
about unnormalized wavefunctions, like someone from the 1950's who's afraid
to use the word ``toilet'' in polite company. Waves are things that superpose,
and this means that they inhabit a vector space (p.~\pageref{lin-alg-vector-space}). One
of the axioms defining a vector space is that we have well-defined operations of
addition and multiplication by a scalar. If we ruled out unnormalized wavefunctions, we would
be violating this requirement. And physically, it is sometimes OK to talk about an unnormalized
state as representing more than one particle, or, in the case of a traveling wave, a steady
flux of particles like the beam of a cathode ray tube.

<% end_sec('what-kind-of-wave') %>

<% begin_sec("Quantum numbers and bra-ket notation",nil,'bra-ket') %>
When we want to describe a wavefunction, it can be cumbersome to write out its equation as in example
\ref{eg:particle-in-box-norm-and-phase}. In that particular example, it's much easier just to
draw a picture or <% eqn_image("eqn-wave2") %> to define some simple numerical label like $N=2$
as we did in problem \ref{hw:symmetric-standing-wave} on p.~\pageref{hw:symmetric-standing-wave}.
A number like this is called a \emph{quantum number},\index{quantum number}
and in this particular example the $N$ quantum number is a wavelength or energy label. (It isn't
a momentum label, because the standing wave is a superposition of traveling waves going in
both directions.)

When we want to refer specifically to the wavefunction that is labeled in these ways, there is
a handy notation involving angle brackets, called the ``bra-ket'' notation.\index{bra-ket notation}
In our example, we
could write something like
\begin{equation*}
  | <% eqn_image("eqn-wave2") %> \rangle
\end{equation*}
or
\begin{equation*}
  |  N=2 \rangle.
\end{equation*}
These are called kets. We can also have notation where the bar and angle bracket are flipped
the other way, like $\langle <% eqn_image("eqn-wave2") %> |$ or $\langle N=2|$. These are called bras. For real-valued
wavefunctions, which are good enough for standing waves, there is no important distinction to be made between
bras and kets. When we sandwich together a bra and a ket, both of which represent the same state, this is a
shorthand way of notating the type of integral that we did in example \ref{eg:particle-in-box-norm-and-phase} for
normalization. That is, if we want to express the requirement that the state be normalized, we can write something like
\begin{equation*}
  \langle <% eqn_image("eqn-wave2") %> | <% eqn_image("eqn-wave2") %> \rangle = 1,
\end{equation*}
which is a lot easier than writing out the whole integral as \\
$\int_0^L |A|^2  \sin^2(2\pi x/L) \der x = 1$.
<% end_sec('bra-ket') %>

<% end_sec() %>

<% begin_sec("Dispersive waves",nil,'dispersive-waves') %>

\index{wave!dispersive}\index{dispersion}
A colleague of mine who teaches chemistry loves to tell the
story about an exceptionally bright student who, when told
of the equation $p=h/\lambda $, protested, ``But when I
derived it, it had a factor of 2!'' The issue that's
involved is a real one, albeit one that could be glossed
over (and is, in most textbooks) without raising any alarms
in the mind of the average student. The present optional
section addresses this point; it is intended for the student
who wishes to delve a little deeper.

Here's how the now-legendary student was presumably
reasoning. We start with the equation $v=f\lambda $, which
is valid for any sine wave, whether it's quantum or
classical. Let's assume we already know $E=hf$, and
are trying to derive the relationship between wavelength and momentum:
\begin{align*}
        \lambda         &=    \frac{v}{f}  \\
                 &=    \frac{vh}{E}  \\
                 &=    \frac{vh}{\frac{1}{2}mv^2}  \\
                 &=    \frac{2h}{mv}  \\
                 &=    \frac{2h}{p}\eqquad.  
\end{align*}

The reasoning seems valid, but the result does contradict
the accepted one, which is after all solidly based on experiment.

<% marg(0) %>
<%
  fig(
    'sine-wave',
    %q{Part of an infinite sine wave.}
  )
%>

\spacebetweenfigs

<%
  fig(
    'sine-wave-pulse',
    %q{A finite-length sine wave.}
  )
%>

\spacebetweenfigs

<%
  fig(
    'beats',
    %q{A beat pattern created by superimposing two sine waves with slightly different wavelengths.}
  )
%>
<% end_marg %>

The mistaken assumption is that we can figure everything out
in terms of pure sine waves. Mathematically, the only wave
that has a perfectly well defined wavelength and frequency
is a sine wave, and not just any sine wave but an infinitely
long sine wave, \figref{sine-wave}.
 The unphysical thing about such a wave
is that it has no leading or trailing edge, so it can never
be said to enter or leave any particular region of space.
Our derivation made use of the velocity, $v$, and if
velocity is to be a meaningful concept, it must tell us how
quickly stuff (mass, energy, momentum, \ldots) is transported
from one region of space to another. Since an infinitely
long sine wave doesn't remove any stuff from one region and
take it to another, the ``velocity of its stuff'' is not a
well defined concept.

Of course the individual wave peaks do travel through space,
and one might think that it would make sense to associate
their speed with the ``speed of stuff,'' but as we will see,
the two velocities are in general unequal when a wave's
velocity depends on wavelength. Such a wave is called a
\emph{dispersive} wave, because a wave pulse consisting of a
superposition of waves of different wavelengths will
separate (disperse) into its separate wavelengths as the
waves move through space at different speeds.  Nearly all
the waves we have encountered have been nondispersive. For
instance, sound waves and light waves (in a vacuum) have
speeds independent of wavelength. A water wave is one good
example of a dispersive wave. Long-wavelength water waves
travel faster, so a ship at sea that encounters a storm
typically sees the long-wavelength parts of the wave first.
When dealing with dispersive waves, we need symbols and
words to distinguish the two \index{velocity!group}\index{velocity!phase}\index{group
velocity}\index{phase velocity}speeds. The speed at which
wave peaks move is called the phase velocity, $v_p$, and the
speed at which ``stuff'' moves is called the group velocity, $v_g$.

An infinite sine wave can only tell us about the phase
velocity, not the group velocity, which is really what we
would be talking about when we refer to the speed of an
electron. If an infinite sine wave is the simplest possible
wave, what's the next best thing? We might think the runner
up in simplicity would be a wave train consisting of a
chopped-off segment of a sine wave, \figref{sine-wave-pulse}. However, this kind
of wave has kinks in it at the end. A simple wave should be
one that we can build by superposing a small number of
infinite sine waves, but a kink can never be produced by
superposing any number of infinitely long sine waves.

Actually the simplest wave that transports stuff from place
to place is the pattern shown in figure 
\figref{beats}. Called a beat
pattern, it is formed by superposing two sine waves whose
wavelengths are similar but not quite the same. If you have
ever heard the pulsating howling sound of musicians in the
process of tuning their instruments to each other, you have
heard a beat pattern. The beat pattern gets stronger and
weaker as the two sine waves go in and out of phase with
each other. The beat pattern has more ``stuff'' (energy, for
example) in the areas where constructive interference
occurs, and less in the regions of cancellation. As the
whole pattern moves through space, stuff is transported from
some regions and into other ones.

If the frequency of the two sine waves differs by 10\%, for
instance, then ten periods will be occur between times when
they are in phase. Another way of saying it is that the
sinusoidal ``envelope'' (the dashed lines in figure \figref{beats}) has
a frequency equal to the difference in frequency between the
two waves. For instance, if the waves had frequencies of 100
Hz and 110 Hz, the frequency of the envelope would be 10 Hz.

To apply similar reasoning to the wavelength, we must define
a quantity $z=1/\lambda $ that relates to wavelength in the
same way that frequency relates to period. In terms of this
new variable, the $z$ of the envelope equals the difference
between the $z's$ of the two sine waves.

The group velocity is the speed at which the envelope moves
through space. Let $\Delta f$ and $\Delta z$ be the
differences between the frequencies and $z's$ of the two
sine waves, which means that they equal the frequency and
$z$ of the envelope. The group velocity is $v_g=f_{envelope}\lambda_{envelope}=\Delta
f/\Delta $z. If $\Delta f$  and $\Delta z$ are sufficiently
small, we can approximate this expression as a derivative,
\begin{equation*}
                v_g         =    \frac{\der f}{\der z}\eqquad.  
\end{equation*}
This expression is usually taken as the definition of the
group velocity for wave patterns that consist of a
superposition of sine waves having a narrow range of
frequencies and wavelengths. In quantum mechanics, with
$f=E/h$ and $z=p/h$, we have $v_g=\der E/\der p$. In the case of a
nonrelativistic electron the relationship between energy and
momentum is $E=p^2/2m$, so the group velocity is $\der E/\der p=p/m=v$,
exactly what it should be. It is only the phase velocity
that differs by a factor of two from what we would have expected,
but the phase velocity is not the physically important thing.

<% end_sec() %>

<% begin_sec("Bound states",nil,'qm-bound-states') %>

\index{states!bound}\index{bound states}
Electrons are at their most interesting when they're in
atoms, that is, when they are bound within a small region of
space. We can understand a great deal about atoms and
molecules based on simple arguments about such bound states,
without going into any of the realistic details of atom. The
simplest model of a bound state is known as the particle in
a box: like a ball on a pool table, the electron feels zero
force while in the interior, but when it reaches an edge it
encounters a wall that pushes back inward on it with a large
force. In particle language, we would describe the electron
as bouncing off of the wall, but this incorrectly assumes
that the electron has a certain path through space. It is
more correct to describe the electron as a wave that
undergoes 100\% reflection at the boundaries of the box.

<% marg(0) %>
<%
  fig(
    'particle-in-a-box',
    %q{Three possible standing-wave patterns for a particle in a box.}
  )
%>
<% end_marg %>

Like generations of physics students before me, I rolled my
eyes when initially introduced to the unrealistic idea of
putting a particle in a box. It seemed completely impractical,
an artificial textbook invention. Today, however, it has
become routine to study electrons in rectangular boxes in
actual laboratory experiments. The ``box'' is actually just
an empty cavity within a solid piece of silicon, amounting
in volume to a few hundred atoms. The methods for creating
these \index{box!particle in a}\index{particle in a
box}electron-in-a-box setups (known as ``\index{quantum
dot}quantum dots'') were a by-product of the development of
technologies for fabricating computer chips.

For simplicity let's imagine a one-dimensional electron in a
box, i.e., we assume that the electron is only free to move
along a line. The resulting standing wave patterns, of which
the first three are shown in the figure, are just like some
of the patterns we encountered with sound waves in musical
instruments. The wave patterns must be zero at the ends of
the box, because we are assuming the walls are impenetrable,
and there should therefore be zero probability of finding
the electron outside the box. Each wave pattern is labeled
according to $n$, the number of peaks and valleys it has. In
quantum physics, these wave patterns are referred to as
``states'' of the particle-in-the-box system.

The following seemingly innocuous observations about the
particle in the box lead us directly to the solutions to
some of the most vexing failures of classical physics:

\noindent\emph{The particle's \index{energy!quantization of for bound
states}energy is quantized (can only have certain values).}
Each wavelength corresponds to a certain momentum, and a
given momentum implies  a definite kinetic energy,
$E=p^2/2m$. (This is the second type of energy quantization
we have encountered. The type we studied previously had to
do with restricting the number of particles to a whole
number, while assuming some specific wavelength and energy
for each particle. This type of quantization refers to the
energies that a single particle can have. Both photons and
matter particles demonstrate both types of quantization
under the appropriate circumstances.)

\noindent\emph{The particle has a minimum kinetic energy.} Long wavelengths
correspond to low momenta and low energies. There can be no
state with an energy lower than that of the $n=1$ state, 
called the ground state.

\noindent\emph{The smaller the space in which the particle is confined, the
higher its kinetic energy must be.} Again, this is because
long wavelengths give lower energies.

<% marg(0) %>
<%
  fig(
    'sirius-spectrum',
    %q{The spectrum of the light from the star Sirius.}
  )
%>
<% end_marg %>

\begin{eg}{Spectra of thin gases}\label{eg:spectra-of-thin-gases}
\index{spectrum!absorption}\index{spectrum!emission}\index{absorption}
\index{emission spectrum}\index{gas!spectrum of}
A fact that was inexplicable by classical physics was
that thin gases absorb and emit light only at certain
wavelengths. This was observed both in earthbound laboratories
and in the spectra of stars. The figure on the left shows
the example of the spectrum of the star \index{Sirius}Sirius,
in which there are ``gap teeth'' at certain wavelengths.
Taking this spectrum as an example, we can give a straightforward
explanation using quantum physics.

   Energy is released in the dense interior of the star, but
the outer layers of the star are thin, so the atoms are far
apart and electrons are confined within individual atoms.
Although their standing-wave patterns are not as simple as
those of the particle in the box, their energies are quantized.

   When a photon is on its way out through the outer layers,
it can be absorbed by an electron in an atom, but only if
the amount of energy it carries happens to be the right
amount to kick the electron from one of the allowed energy
levels to one of the higher levels. The photon energies that
are missing from the spectrum are the ones that equal the
difference in energy between two electron energy levels.
(The most prominent of the absorption lines in Sirius's
spectrum are absorption lines of the hydrogen atom.)
\end{eg}

\begin{eg}{The stability of atoms}
   In many \index{Star Trek}Star Trek episodes the
Enterprise, in orbit around a planet, suddenly lost engine
power and began spiraling down toward the planet's surface.
This was utter nonsense, of course, due to conservation of
energy: the ship had no way of getting rid of energy, so it
did not need the engines to replenish it.

   Consider, however, the electron in an atom as it orbits
the nucleus. The electron \emph{does} have a way to release
energy:  it has an acceleration due to its continuously
changing direction of motion, and according to classical
physics, any accelerating charged particle emits electromagnetic
waves. According to classical physics, atoms should collapse!

   The solution lies in the observation that a bound state
has a minimum energy. An electron in one of the higher-energy
atomic states can and does emit photons and hop down step by
step in energy. But once it is in the ground state, it
cannot emit a photon because there is no lower-energy
state for it to go to.
\end{eg}

<% marg(0) %>
<%
  fig(
    'h-molecule',
    %q{%
      Two hydrogen atoms bond to form
       an $\zu{H}_2$ molecule. In the molecule, the two electrons' wave patterns overlap
      , and are about twice as wide.
    }
  )
%>
<% end_marg %>

\begin{eg}{Chemical bonds}\label{h2-bond}\index{chemical bonds!quantum explanation for hydrogen}
I began this section with a classical argument that chemical
bonds, as in an $\zu{H}_2$ molecule, should not exist. Quantum
physics explains why this type of bonding does in fact
occur. When the atoms are next to each other, the electrons
are shared between them. The ``box'' is about twice as wide,
and a larger box allows a smaller energy. Energy is required
in order to separate the atoms. (A qualitatively different
type of bonding is discussed on page
\pageref{ionicbonds}.m4_ifelse(__sn,1,[: Example \ref{eg:h2-details} on page 
\pageref{eg:h2-details} revisits the $\zu{H}_2$ bond in more detail.:],[::]))
\end{eg}

\startdqs

__incl(dq/dineutron)

\begin{dq}
The following table shows the energy gap between the
ground state and the first excited state for four nuclei, in
units of picojoules. (The nuclei were chosen to be ones
that have similar structures, e.g., they are all spherical in shape.)

\begin{tabular}{ll}
    nucleus    &  energy gap (picojoules)\\
    $^4\zu{He}$       & 3.234\\
    $^{16}\zu{O}$     & 0.968\\
    $^{40}\zu{Ca}$    & 0.536\\
    $^{208}\zu{Pb}$   & 0.418\\
\end{tabular}

\noindent Explain the trend in the data.
\end{dq}

<% end_sec('qm-bound-states') %>

<% begin_sec('The uncertainty principle',nil,'uncertainty-principle') %>

\index{Heisenberg!Werner}\index{Heisenberg uncertainty
principle}\index{uncertainty principle}

<% begin_sec("Eliminating randomness through measurement?") %>

A common reaction to quantum physics, among both early-twentieth-century
physicists and modern students, is that we should be able to
get rid of randomness through accurate measurement. If I
say, for example, that it is meaningless to discuss the path
of a photon or an electron, one might suggest that we simply
measure the particle's position and velocity many times in a
row. This series of snapshots would amount to a description of its path.

A practical objection to this plan is that the process of
measurement will have an effect on the thing we are trying
to measure. This may not be of much concern, for example,
when a traffic cop measure's your car's motion with a radar
gun, because the energy and momentum of the radar pulses are
insufficient to change the car's motion significantly. But
on the subatomic scale it is a very real problem. Making a
videotape through a microscope of an electron orbiting a
nucleus is not just difficult, it is theoretically
impossible. The video camera makes pictures of things using
light that has bounced off them and come into the camera. If
even a single photon of visible light was to bounce off of
the electron we were trying to study, the electron's recoil
would be enough to change its behavior significantly.  

<% end_sec() %>

<% marg(100) %>
<%
  fig(
    'heisenberg',
    %q{Werner Heisenberg (1901-1976). Heisenberg helped to develop the foundations of quantum mechanics,
       including the Heisenberg uncertainty principle. He was the scientific leader of
       the Nazi atomic-bomb program up until its cancellation in 1942, when the
       military decided that it was too ambitious a project
       to undertake in wartime, and too unlikely to produce results.}
  )
%>
<% end_marg %>

<% begin_sec("The Heisenberg uncertainty principle") %>

This insight, that measurement changes the thing being
measured, is the kind of idea that clove-cigarette-smoking
intellectuals outside of the physical sciences like to claim
they knew all along. If only, they say, the physicists had
made more of a habit of reading literary journals, they
could have saved a lot of work. The anthropologist Margaret
Mead has recently been accused of inadvertently encouraging
her teenaged Samoan informants to exaggerate the freedom of
youthful sexual experimentation in their society. If this is
considered a damning critique of her work, it is because she
could have done better: other anthropologists claim to have
been able to eliminate the observer-as-participant problem
and collect untainted data.

The German physicist Werner Heisenberg, however, showed that
in quantum physics, \emph{any} measuring technique runs into
a brick wall when we try to improve its accuracy beyond a
certain point. Heisenberg showed that the limitation is a
question of \emph{what there is to be known}, even in
principle, about the system itself, not of the ability or
inability of a specific measuring device to ferret out
information that is knowable but not previously hidden.

Suppose, for example, that we have constructed an electron
in a box (quantum dot) setup in our laboratory, and we are
able to adjust the length $L$ of the box as desired. All the
standing wave patterns pretty much fill the box, so our
knowledge of the electron's position is of limited accuracy.
If we write $\Delta x$ for the range of uncertainty in our
knowledge of its position, then $\Delta x$ is roughly the
same as the length of the box:\label{heisenberg-argument}
\begin{equation*}
        \Delta x \approx L
\end{equation*}
If we wish to know its position more accurately, we can
certainly squeeze it into a smaller space by reducing $L$,
but this has an unintended side-effect. A standing wave is
really a superposition of two traveling waves going in
opposite directions. The equation $p=h/\lambda $ really only
gives the magnitude of the momentum vector, not its
direction, so we should really interpret the wave as a 50/50
mixture of a right-going wave with momentum $p=h/\lambda $
and a left-going one with momentum $p=-h/\lambda $. The
uncertainty in our knowledge of the electron's momentum is
$\Delta p=2h/\lambda$, covering the range between these two
values. Even if we make sure the electron is in the ground
state, whose wavelength $\lambda =2L$ is the longest
possible, we have an uncertainty in momentum of 
$\Delta p=h/L$. In general, we find
\begin{equation*}
        \Delta p \gtrsim h/L\eqquad,
\end{equation*}
with equality for the ground state and inequality for the
higher-energy states. Thus if we reduce $L$ to improve our
knowledge of the electron's position, we do so at the cost
of knowing less about its momentum. This trade-off is neatly
summarized by multiplying the two equations to give
\begin{equation*}
        \Delta p\Delta x \gtrsim h\eqquad.
\end{equation*}
Although we have derived this in the special case of a
particle in a box, it is an example of a principle of
more general validity:
\begin{important}[The Heisenberg uncertainty principle]
It is not possible, even in principle, to know the momentum
and the position of a particle simultaneously and with
perfect accuracy. The uncertainties in these two quantities
are always such that $\Delta p\Delta x \gtrsim h$.
\end{important}
\noindent (This approximation can be made into a strict inequality,
$\Delta p\Delta x>h/4\pi$, but only with more careful
definitions, which we will not bother with.)

Note that although I encouraged you to think of this
derivation in terms of a specific real-world system, the
quantum dot, no reference was ever made to any specific
laboratory equipment or procedures. The argument is simply
that we cannot \emph{know} the particle's position very
accurately unless it \emph{has} a very well defined
position, it cannot have a very well defined position unless
its wave-pattern covers only a very small amount of space,
and its wave-pattern cannot be thus compressed without
giving it a short wavelength and a correspondingly uncertain
momentum. The uncertainty principle is therefore a
restriction on how much there is to know about a particle,
not just on what we can know about it with a certain technique.

\begin{eg}{An estimate for electrons in atoms}
\egquestion A typical energy for an electron in an atom is on
the order of $(\text{1 volt})\cdot e$, which corresponds to a speed of
about 1\% of the speed of light. If a typical atom has a
size on the order of 0.1 nm, how close are the electrons to
the limit imposed by the uncertainty principle?

\eganswer If we assume the electron moves in all directions
with equal probability, the uncertainty in its momentum is
roughly twice its typical momentum. This is only an order-of-magnitude
estimate, so we take $\Delta p$ to be the same as a typical momentum:
\begin{align*}
 \Delta p \Delta x         &=    p_{typical} \Delta x   \\
                         &=    (m_{electron}) (0.01c) (0.1\times10^{-9}\ \munit)  \\
                         &=    3\times 10^{-34}\ \zu{J}\unitdot\zu{s}  
\end{align*}
This is on the same order of magnitude as Planck's constant,
so evidently the electron is ``right up against the wall.''
(The fact that it is somewhat less than $h$ is of no concern
since this was only an estimate, and we have not stated the
uncertainty principle in its most exact form.)
\end{eg}

<% self_check('smallh',<<-'SELF_CHECK'

If we were to apply the uncertainty principle to human-scale
objects, what would be the significance of the small
numerical value of Planck's constant?
  SELF_CHECK
  ) %>

<% end_sec() %>

\startdqs

\begin{dq}
Compare $\Delta p$  and $\Delta x$ for the two lowest
energy levels of the one-dimensional particle in a box, and
discuss how this relates to the uncertainty principle.
\end{dq}

\begin{dq}
On a graph of $\Delta p$ versus $\Delta $x, sketch the
regions that are allowed and forbidden by the Heisenberg
uncertainty principle. Interpret the graph: Where does an
atom lie on it? An elephant? Can either $p$ or $x$ be
measured with perfect accuracy if we don't care about the other?
\end{dq}

<% end_sec() %>

<% begin_sec("Decoherence",nil,'decoherence') %>
\index{decoherence}
Starting around 1970, physicists began to realize that ideas involving a loss of coherence,
or ``decoherence,'' could help to explain some things about quantum mechanics that
had previously seemed mysterious.  The classical notions of coherence and coherence length
were described in sec.~\ref{sec:coherence}, p.~\pageref{sec:coherence}, and quantum-mechanical
decoherence was briefly introduced on p.~\pageref{decoherence-brief}.

One mystery was the fact that it is difficult to
demonstrate wave interference effects with large objects. This is partly because
the wavelength $\lambda=h/p=h/mv$ tends to be small for an object with a large mass (example \ref{eg:cat-wavelength}, p.~\pageref{eg:cat-wavelength}).
But even taking this into account, we do not seem to have much luck observing,
for example, double-slit diffraction of very large molecules, even
when we use slits with appropriate dimensions and a detector with a good enough angular
resolution.

In the early days of quantum mechanics, people like Bohr and Heisenberg imagined that
there was simply a clear division between the macroscopic and microscopic worlds.
Big things and small things just had different rules: Newton's laws in one case, quantum
mechanics in the other. But this is no longer a tenable position,
because we now know that there is no limit on the distance scales over which quantum-mechanical
behavior can occur. For example, a communication satellite carried out a demonstration
in 2017 in which a coherence length of 1200 km was demonstrated using 
photons.\footnote{Yin \emph{et al.}, \url{arxiv.org/abs/1707.01339}}

<%
  fig(
    'decohering-wave-packet',
    %q{%
      A large molecule such as the one in the Eibenberger experiment is represented by its
      wavepacket. 
      As the molecule starts out, its coherence length, shown by the arrows, is quite long.
      As it flies to the right, it is bombarded by infrared photons, which
      randomize its phase, causing its coherence length to shorten exponentially: by a factor
      of two in the second panel, and by a further factor of two in the final one. When the
      packet enters the double slit, its coherence length is on the same order of magnitude
      as the slits' spacing $d$, which will worsen but not entirely eliminate the observability
      of interference fringes.
      (This is only a schematic representation, with the wavepacket shown as being
      many orders of magnitude bigger than its actual size in relation to the vacuum chamber.
      Also, the real experiment used a reflecting grating, not a transmitting double slit.)
    },
    {
      'width'=>'wide',
      'sidecaption'=>true
    }
  )
%>

The insight about decoherence was the following. Consider the most massive material
object that has so far been successfully dif\-fracted through a grating,
which was a molecule consisting of about 810 atoms in an experiment by
Eibenberger \emph{et al.} in 2013.\footnote{\url{arxiv.org/abs/1310.8343}}
While this molecule was propagating through the apparatus as a wave, the experimenters
needed to keep it from simply being stopped by a collision with an air molecule.
For this reason, they had to do the experiment inside a vacuum chamber, with an extremely
good vacuum. But even then, the molecule was being bombarded by photons of infrared light
emitted from the walls of the chamber. The effect of this bombardment is to disrupt the
molecule's wavefunction and reduce its coherence length (p.~\pageref{fig:coherence}).

This decoherence effect was the reason
that the experiment was limited to molecules of the size they used. Even though the molecules
took only about 400 nanoseconds to fly through the apparatus, there was a significant amount of
decoherence. A larger molecule would have been a bigger target for photons and would have undergone
decoherence more quickly, making interference unobservable.

<% end_sec('decoherence') %>

<% begin_sec("A crude model of the hydrogen atom",nil,'crude-hydrogen') %>\index{hydrogen atom!energies of states in}
<% begin_sec("Modeling",nil,'crude-hydrogen-model') %>
Probably the most important reason for wanting to understand quantum physics is in order
to understand the atom. As tiny as they may be, atoms can be pretty complicated. Let's think
about ways to simplify them --- even at the risk of oversimplifying.
Start with hydrogen, because it's the simplest of all atoms, consisting of only one electron and one proton. Because
the proton is 1800 times more massive than the electron, it's a good approximation
to imagine that it stays at rest while the electron moves around, influenced by the proton's static
electric field. This is good: we've reduced our problem to the discussion of just one particle.

Classically, the electron moves around in the proton's electric field, speeding up, slowing down,
and never straying farther than a certain distance $r$ because it doesn't have enough energy.
As it does all these things, its momentum changes, so if we mix in a little quantum mechanics,
$p=\hbar k$ then tells us that its wavenumber
and wavelength change. So its wavefunction can't be a nice, simple sine wave with a single well-defined wavelength. We expect it to look
more like this: <% eqn_image("eqn-hydrogen") %>. When the electron is close to the proton, its electrical
energy is low, so its kinetic energy is high, and its wavelength is short. When it's farther out, its
wavelength is longer. In chapter \ref{ch:schrodinger} we'll talk in more detail about how to handle
this sort of thing.

But for now we're going to do something much more crude, which is to \emph{approximate} the electron's
wavefunction as a sine wave like this one <% eqn_image("eqn-hydrogen-sine") %>. We take the nodes and
antinodes and make them nice and regular: all evenly spaced and equal in amplitude, and filling the
whole space available to the electron based on the amount of energy it has.

Also, by drawing the wavefunctions on a piece of paper in this way, I've snuck yet another crude
approximation past you. We're treating the hydrogen atom as one-dimensional, $\Psi(x)$ rather
than $\Psi(x,y,z)$. The wave fills in the line segment $-r\le x\le r$, where $r$ is the maximum
distance. The drawings also implicitly imply that $\Psi$ is a function whose outputs are real
rather than complex, but we can get away with this because this is a standing wave.
<% end_sec('crude-hydrogen-model') %>

<% begin_sec("Estimation of the energy levels",nil,'crude-hydrogen-est') %>
Let $n$ be the same quantum number defined in problem \ref{hw:symmetric-standing-wave}, p.~\pageref{hw:symmetric-standing-wave},
and in sec.~\ref{subsec:bra-ket}, p.~\pageref{subsec:bra-ket}, i.e., it's the number of antinodes or the
number of half-wavelengths. In the examples drawn above, $n=9$. The ground state has $n=1$.
Because $n$ is the number of half-wavelengths, the wavelength is $4r/n$, but because this
whole calculation is so crude, we're going to throw away factors like the 4, and just say that
\begin{equation*}
  \lambda\sim r/n.
\end{equation*}

If $r$ had the same value for every standing-wave pattern, then we'd
essentially be solving the particle-in-a-box problem, but there is a fundamental difference here,
which is that we don't have a box of a fixed size with impenetrable walls. If we did, then
it would be impossible to separate the electron from the proton, whereas in real life we know
that this can be done by putting in a certain amount of energy to ionize the atom.
The force keeping
the electron bound isn't an infinite force encountered when it bounces off of a wall, it's
the attractive electrical force from the nucleus. If we put more energy into the electron,
it's like throwing a ball upward with a higher energy --- it will get farther out before
coming back down.

<% marg(80) %>
<%
  fig(
    'hydrogen-versus-box',
    %q{%
      The energy levels of a particle in a box, contrasted with those of the hydrogen atom.
    }
  )
%>
<% end_marg %>
Figure \figref{hydrogen-versus-box} shows how we expect this to turn out. 
In the hydrogen atom, we expect $r$ to increase as we
go to states of higher energy. This tends to keep the wavelengths of the high energy
states from getting too short, reducing their kinetic energy. The closer and closer
crowding of the energy levels in hydrogen also makes sense because we know that there
is a certain energy that would be enough to make the electron escape completely, and
therefore the sequence of bound states cannot extend above that energy.

Now let's crunch some numbers. When the electron is at the maximum classically allowed distance $r$ from the proton, it
has zero kinetic energy.
Thus when the
electron is at distance $r$, its energy is purely electrical,
\begin{equation}\label{eqn:h-atom-energy}
  E    = -\frac{ke^2}{r},
\end{equation}
where $k$ is the Coulomb constant (not the wavenumber).  The zero-level of the electrical energy
scale is chosen to be the energy of an electron and a proton
that are infinitely far apart. With this choice, negative
energies correspond to bound states and positive energies to unbound ones.

Finally we assume that the typical kinetic energy of the
electron is on the same order of magnitude as the absolute
value of its total energy. (This is true to within a factor
of two for a typical classical system like a planet in a
circular orbit around the sun.) We then have
\begin{subequations}
\renewcommand{\theequation}{\theparentequation}
\begin{align}
        \text{absolute}&\text{ value of total energy} \\
        &= \frac{ke^2}{r} \notag \\
        &\sim K \notag \\ 
        &= p^2/2m \notag \\
        &= (h/\lambda)^2/2m \notag \\
        &\sim h^2n^2/2mr^2 \notag
\end{align}
\end{subequations}
We now solve the equation $ke^2/r \sim h^2n^2 / 2mr^2$ for $r$
and throw away numerical factors we can't hope to have
gotten right, yielding
\begin{equation}\label{eqn:h-atom-r-eqn}
        r \sim \frac{h^2n^2}{mke^2}\eqquad.
\end{equation}

Plugging $n=1$ into this equation gives $r=2$ nm, which is
indeed on the right order of magnitude compared to the observed sizes of atoms. Finally we combine
equations [\ref{eqn:h-atom-r-eqn}] and [\ref{eqn:h-atom-energy}] to find
\begin{equation*}
        E \sim -\frac{mk^2e^4}{h^2}\cdot\frac{1}{n^2} \qquad \text{[result of crude treatment]}
\end{equation*}
which turns out to be correct except for the numerical factors we
never aimed to find.\label{end-approx-hydrogen-energies} The exact result is
\begin{equation*}
 E = -\frac{mk^2e^4}{2\hbar^2}\cdot\frac{1}{n^2}, \qquad \text{[exact result]}
\end{equation*}
which is different by a factor of $2\pi^2$. It might seem incredible that such a crude approximation
would provide a result so close to the correct one. However, the factor in front is constrained
by the requirement that the result have units of energy. All of the exponents in this factor have
to be as they are just based on units. For the ground state, $n=1$, we will show in example
\ref{eg:h-ground-exact}, p.~\pageref{eg:h-ground-exact}, that the exact result is correct.

<% end_sec('crude-hydrogen-est') %>

<% begin_sec("Comparison with experiment",nil,'hydrogen-exp') %>
The experimental technique for measuring the energy levels
of an atom accurately is spectroscopy: the study of the
spectrum of light emitted (or absorbed) by the atom. Only
photons with certain energies can be emitted or absorbed by
a hydrogen atom, for example, since the amount of energy
gained or lost by the atom must equal the difference in
energy between the atom's initial and final states.
Spectroscopy had become a highly developed art
several decades before Einstein even proposed the photon,
and the Swiss spectroscopist Johann \index{Balmer, Johann}Balmer
determined in 1885 that there was a simple equation that
gave all the wavelengths emitted by hydrogen. In modern
terms, we think of the photon wavelengths merely as indirect
evidence about the underlying energy levels of the atom, and
we rework Balmer's result into an equation for these
atomic energy levels:
\begin{equation*}
                E_n    =  -\frac{A}{n^2}\eqquad,  
\end{equation*}
where $A=2.2\times10^{-18}\ \zu{J}$.
About 30 years later, the constant $A$ was explained to be the constant factor in the
equation at the end of the previous section.
<% end_sec('hydrogen-exp') %>

\startdqs

\begin{dq}
States of hydrogen with $n$ greater than about 10 are
never observed in the sun. Why might this be?
\end{dq}

\begin{dq}
Sketch graphs of $r$ and $E$ versus $n$ for the hydrogen,
and compare with analogous graphs for the one-dimensional particle in a box.
\end{dq}


<% end_sec('crude-hydrogen') %>



<% begin_hw_sec(vfill:true) %>


<% end_hw_sec %>


<% end_chapter() %>
